# 2.8 Subspaces of R {.unnumbered}

## Column Space

::: {#thm-2-8-13}

## Theorem 13

The pivot columns of a matrix $A$ form a basis for the column space of $A$.
:::

::: {.proof}

## Proof of [@thm-2-8-13]

Suppose $B$ is the RREF of $A$, then there's a sequence of elementary matrices, $E_1,\dots,E_p$, where $E_p\cdots E_1 A=B$. Let $E=E_p\cdots E_1$, so we have $EA=B$.

Denote the columns of $A$ and $B$ as $\mathbf{a}_k$ and $\mathbf{b}_k$, respectively. The linear relationship in the columns of $A$ is preserved in the columns of $B$ because
\begin{align}
EA &= E\begin{bmatrix}\mathbf{a}_1 & \cdots & \mathbf{a}_n\end{bmatrix} \\
&= \begin{bmatrix}E\mathbf{a}_1 & \cdots & E\mathbf{a}_n\end{bmatrix} \\
&= \begin{bmatrix}\mathbf{b}_1 & \cdots & \mathbf{b}_n\end{bmatrix}
\end{align}
Thus, if $\mathbf{a}_k = \sum_{j\neq k} c_j\mathbf{a}_j$, $\mathbf{b}_k=E\mathbf{a}_k=E\sum_{j\neq k} c_j \mathbf{a}_j=\sum_{j\neq k} c_j E\mathbf{a}_j=\sum_{j\neq k} c_j\mathbf{b}_j$.

Conversely, since $E$ is invertible, we can prove the linear relationship in the columns of $B$ is preserved in the columns of $A$ by replacing $E$ in the above proof with $E^{-1}$ and swapping $A$ and $B$.

Therefore, we have proved the linear relationship is the same in both $A$ and $B$.
:::

::: {.remark}

It's easy to show that every non-pivot column can be written as a linear combination of the prior pivot columns by the definition of RREF, so we can safely remove the non-pivot columns from the set of the columns without changing the span. Thus, we have completely proved [@thm-2-8-13].

Although the pivot columns of $A$ form a basis for $\operatorname{Col} A$, it's not the only basis that can be formed from the columns. If we do row reduction in a different order of the columns, the pivot columns may be different.

<!-- There's an interesting question: how many ways are there to form a basis of $\operatorname{Col} A$ using the columns of $A$? -->
:::

<!-- ::: {.callout-tip}

## How many ways?

This is a very generalized question, so to my current knowledge, I doubt that there's a clean mathematical solution. Still, we can simplify the question and use computers to solve it. The following solution can be implemented in a time complexity of $\mathcal{O}(n^3)$ for row reduction and $\mathcal{O}(n^2)$ for the main algorithm.

First of all, a set of vectors is no more than a set of linearly independent vectors plus some vectors that can be expressed as a linear combination of the linearly independent set of vectors. Denote the linearly independent set as $S$, which is obviously a basis.

Let's consider the simplest case, adding a non-zero vector that is a multiple of a vector in $S$. The answer is simply two in this case since we can either replace the original vector or not.

If we now insert $\mathbf{v}=\mathbf{v}_1+\mathbf{v}_2$, where $\mathbf{v}_1,\mathbf{v}_2\in S$, the answer is three because $\mathbf{v}+(-\mathbf{v}_1)+(-\mathbf{v}_2)=0$ indicates that we can linearly combine the third if we have two in the basis.

Thus, if we are to add a vector $\mathbf{v}$ which can be expressed as a linear combination of $k$ vectors in $S$ with non-zero coefficients, the answer is $k+1$.
::: -->

## Null Space

::: {#def-2-8-nullspace}

## Null Space

The **null space** of a matrix $A$ is the set $\operatorname{Nul} A$ of all solutions of the homogeneous equation $A\mathbf{x}=\mathbf{0}$.
:::

::: {#prp-2-8-nullspace-1}

Row operations don't change null space.
:::

::: {.proof}

## Proof of [@prp-2-8-nullspace-1]

This is how we solve equations.
:::

::: {#prp-2-8-nullspace-2}

Row operations don't change row space, and the non-zero rows of the REF of a matrix form a basis of the row space.
:::

::: {#prp-2-8-nullspace-3}

By some observation, $\operatorname{Nul} A=\bigcap_{i=1}^m \operatorname{Nul}\operatorname{row}_i(A)$.
:::

::: {.proof}

## Proof to [@prp-2-8-nullspace-3] (Informal)

The solution to the system of linear equations is also the solution to each linear equation.
:::

::: {#prp-2-8-nullspace-4}

Each row is orthogonal to the null space of the row. The row space is orthogonal to the null space.
:::

::: {.proof}

Since $A\mathbf{x}=\mathbf{0}$, by the rule of matrix multiplication, $\operatorname{row}_i(A)\mathbf{x}=0$ for all $1\leq i\leq m$. Thus, each vector $x$ in $\operatorname{Nul} A$ is orthogonal to the row space.

The first sentence is a special case of the second.
:::

::: {.remark}

Let's construct a basis for the null space of a row of $A$.

The null space of the $i$-th row is $\{(x_1,\dots,x_n):r_1x_1+\dots+r_nx_n=0\}$, where $r_j$ is the $j$-th column of the $i$-th row of $A$.

Set $x_2=1$ and $x_3,\dots,x_n=0$, we have $(-\frac{r_2}{r_1},1,0,\dots,0)$ as one vector in the basis.

Similarly, we can obtain the other $n-2$ vectors in the basis.

$$(-\frac{r_3}{r_1},0,1,0,0,\dots,0)$$
$$(-\frac{r_4}{r_1},0,0,1,0,\dots,0)$$
$$\cdots$$
$$(-\frac{r_n}{r_1},0,0,0,0,\dots,1)$$
:::

## Linear Transformation

::: {.remark}

Consider a linear transformation $T: \mathbb{R}^n \to \mathbb{R}^m$, where $T(\mathbf{x})=A\mathbf{x}$. $\operatorname{Col}A$ is the range of the transformation, and $\operatorname{Nul}A$ is the set of $\mathbf{x}\in\mathbb{R}^n$ whose image under transformation $T$ is $\mathbf{0}$. Thus, for every $T(\mathbf{u})=\mathbf{v}$ and $\mathbf{p}\in\operatorname{Nul}A$, $T(\mathbf{u}+\mathbf{p})=\mathbf{v}$.
:::