[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Some Notes in Linear Algebra",
    "section": "",
    "text": "Preface\nThis is a book about some notes when the author (currently a 12th grader) is taking an honor course in college linear algebra in high school. Subjected to the author’s knowledge level and the fact that the course targets high school students, this book may include some inaccurate parts. The author will try to avoid making mistakes, but in case you find a potential error while reading, please consult your instructor (or ChatGPT, Grok, DeepSeek, etc.).\nThe textbook used in the course, Linear Algebra and Its Applications 5th Edition (Lay, Lay, and McDonald 2016), serves as the basis of this book.\nThe indices of theorems, exercises, etc. may not be the same in the textbook. For example, “Theorem 1.1 (Theorem 7)” under Section 1.7 refers to Theorem 7 in Section 1.7 in the textbook, but it will sometimes be refered to as Theorem 1.1 in this book. Be cautious of the actual index.\nA reminder before all: do not try to prove definitions. They are called definitions because they are defined. What we prove are theorems.\n\n\n\n\nLay, David C, Steven R Lay, and Judith McDonald. 2016. Linear Algebra and Its Applications. 5th ed. Pearson.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part1/chap1.7.html",
    "href": "part1/chap1.7.html",
    "title": "1.7 Linear Independence",
    "section": "",
    "text": "Definition and Characterization",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "1.7 Linear Independence"
    ]
  },
  {
    "objectID": "part1/chap1.7.html#definition-and-characterization",
    "href": "part1/chap1.7.html#definition-and-characterization",
    "title": "1.7 Linear Independence",
    "section": "",
    "text": "NoteDefinition (Linear Independence)\n\n\n\n\nDefinition 1 (Linear Independence) An indexed set of vectors \\(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) in \\(\\mathbb{R}^m\\) is said to be linearly independent if the vector equation \\[x_1\\mathbf{v}_1+x_2\\mathbf{v}_2+\\dots+x_n\\mathbf{v}_n=\\mathbf{0}\\] has only the trivial solution (\\(\\mathbf{x}=\\mathbf{0}\\)). Otherwise, it is said to be linearly dependent.\n\n\n\n\n\n\n\n\n\n\nTheorem 1 (Theorem 7: Characterization of Linearly Dependent Sets) An indexed set \\(S = \\{\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\}\\) of two or more vectors is linearly dependent if and only if at least one of the vectors in \\(S\\) is a linear combination of the others. In fact, if \\(S\\) is linearly dependent and \\(\\mathbf{v}_1 \\neq \\mathbf{0}\\), then some \\(\\mathbf{v}_j\\) (with \\(j &gt; 1\\)) is a linear combination of the preceding vectors, \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_{j-1}\\).\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof of Theorem 1)\n\n\n\n\n\n\nProof (Proof of Theorem 1). Let “in fact” separate the theorem into two parts. Consider the first part.\n\nIf there is a vector that is a linear combination of the other vectors, \\[\\mathbf{v}_i=c_1\\mathbf{v}_1+\\dots+c_{i-1}\\mathbf{v}_{i-1}+c_{i+1}\\mathbf{v}_{i+1}+\\dots+c_n\\mathbf{v}_n,\\] move the terms into one side: \\[c_1\\mathbf{v}_1+\\dots+c_{i-1}\\mathbf{v}_{i-1}+(-1)\\mathbf{v}_i+c_{i+1}\\mathbf{v}_{i+1}+\\dots+c_n\\mathbf{v}_n=\\mathbf{0}.\\] Thus, there is a nontrivial solution for \\(\\mathbf{c}\\), where \\(c_i=-1\\), so the set is linearly dependent.\nIf the set is linearly dependent, the vector equation \\(c_1\\mathbf{v}_1+\\dots+c_n\\mathbf{v}_n=\\mathbf{0}\\) has nontrivial solution of \\(\\mathbf{c}\\). There exists an index \\(i\\) where \\(c_i \\neq 0\\). Move the \\(i\\)-th term to the left hand side and the other terms to the right hand side: \\[-c_i\\mathbf{v}_i=c_1\\mathbf{v}_1+\\dots+c_{i-1}\\mathbf{v}_{i-1}+c_{i+1}\\mathbf{v}_{i+1}+\\dots+c_n\\mathbf{v}_n.\\] Divide both sides by \\(-c_i\\): \\[\\mathbf{v}_i=\\frac{c_1}{-c_i}\\mathbf{v}_1+\\dots+\\frac{c_{i-1}}{-c_i}\\mathbf{v}_{i-1}+\\frac{c_{i+1}}{-c_i}\\mathbf{v}_{i+1}+\\dots+\\frac{c_n}{-c_i}\\mathbf{v}_n.\\] Hence, \\(\\mathbf{v}_i\\) is a linear combination of the other vectors in the set.\n\nWe have now proved the first part of the theorem. Let’s prove the second part by induction and contradiction.\nIf \\(\\mathbf{v}_j = \\mathbf{0}\\) (\\(1 &lt; j \\leq n\\)), \\(\\mathbf{v}_j\\) can be expressed as \\(0\\cdot\\mathbf{v}_1+\\dots+0\\cdot\\mathbf{v}_{j-1}\\), so we only consider situations that, for all \\(1 \\leq j \\leq n\\), \\(\\mathbf{v}_j\\neq\\mathbf{0}\\).\nLet \\(A_k=\\begin{bmatrix} \\mathbf{v}_1 & \\cdots & \\mathbf{v}_k \\end{bmatrix}\\), where \\(1 \\leq k \\leq n\\), so \\(A \\in \\mathbb{R}^{m \\times k}\\).\nThe reduced row echelon form (RREF) of \\(A_1\\) is \\(\\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\). If \\(\\mathbf{v}_2\\) is not a linear combination of \\(\\mathbf{v}_1\\), \\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}\\) is linearly independent. Thus, the RREF of \\(A_2\\) is \\(\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\end{bmatrix}\\). Similarly, the RREF of \\(A_3\\) needs to be \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ \\vdots & \\vdots & \\vdots \\\\ 0 & 0 & 0 \\end{bmatrix}\\), since if there is no pivot position in the third column, the solution of \\(A_3\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\mathbf{0}\\) can be written as \\(\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=x_3\\begin{bmatrix}u_1\\\\u_2\\\\1\\end{bmatrix}\\), where \\(x_3,u_1,u_2\\in\\mathbb{R}\\). Thus, the RREF of \\(A_k\\) must be in the form of \\(\\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\). When \\(k=n\\), the matrix equation \\(A_n\\mathbf{x}=\\mathbf{0}\\) has only the trivial solution because every column has a pivot position. Hence, \\(S\\) is linearly independent, contradicting the stated condition that \\(S\\) is linearly dependent.\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. When the author first learned about linear independence, the characteristic (Theorem 1), rather than the definition (Definition 1), was taught first. That is, if none of the vectors in a set are a linear combination of the other vectors, the set is linearly independent. This characterization may more intuitively illustrates the name “linear independence” than the definition does, as every vector is “independent” of the other vectors.",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "1.7 Linear Independence"
    ]
  },
  {
    "objectID": "part1/chap1.7.html#pivot-columns-and-linear-independence",
    "href": "part1/chap1.7.html#pivot-columns-and-linear-independence",
    "title": "1.7 Linear Independence",
    "section": "Pivot Columns and Linear Independence",
    "text": "Pivot Columns and Linear Independence\n\n\n\n\n\n\n\nExercise 1 (T27) How many pivot columns must a \\(7 \\times 5\\) matrix have if its columns are linearly independent?\n\n\n\n\n\n\n\n\n\n\nNoteSolution (Solution to Exercise 1)\n\n\n\n\n\n\nSolution 1 (Solution to Exercise 1). Recall the definition of linear indenpendence: \\(A\\mathbf{x}=\\mathbf{0}\\) has only the trivial solution (\\(\\mathbf{x}=\\mathbf{0}\\)). There must be 5 pivot columns in \\(A\\)’s row echelon form (REF). Otherwise, the free variable(s) will produce solutions other than \\(\\mathbf{0}\\).\nIf we generalize the situation to \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(n\\) pivot columns are required to avoid free variables. Therefore, the columns of \\(A\\) are necessarily linearly dependent when \\(n &gt; m\\), as there are no enough rows to generate \\(n\\) pivot columns (Theorem 8).\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. What if there are columns without a pivot position?\nLet the \\(k\\)-th column be one of those columns. Then, \\(x_k\\) is a free variable. According to the proof of Theorem 1, \\(\\mathbf{v}_k \\in \\text{span}\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_{k-1}\\}\\). In addition, we can observe the solution of the matrix equation \\(A\\mathbf{x}=\\mathbf{0}\\), \\(\\mathbf{x}=x_k\\mathbf{u}+x_{k_1}\\mathbf{u}_1+\\dots+x_{k_t}\\mathbf{u}_t\\), where \\(\\mathbf{u},\\mathbf{u}_1,\\dots,\\mathbf{u}_t\\) are the vectors obtained from the RREF and \\(x_{k_1},\\dots,x_{k_t}\\) are other free variables. Set \\(x_{k_1},\\dots,x_{k_t}\\) equal to zero: \\[\\mathbf{x}=x_k\\mathbf{u}.\\] Substitute this into the vector equation, \\(x_1\\mathbf{v}_1+\\dots+x_n\\mathbf{v}_n=\\mathbf{0}\\): \\[x_k u_1\\mathbf{v}_1+\\dots+x_k u_k\\mathbf{v}_k+\\dots+x_k u_n\\mathbf{v}_n=\\mathbf{0}.\\] Since \\(x_k\\) is a free variable, we can divide the equation by \\(x_k\\) if \\(x_k \\neq 0\\). Then, separate the \\(k\\)-th term to one side (\\(u_k=1\\)): \\[\\mathbf{v}_k=(-u_1)\\mathbf{v}_1+\\dots+(-u_n)\\mathbf{v}_n.\\] Since the matrix has been simplified to its RREF, \\(u_{k+1},\\dots,u_n\\) all equal zero. Then, \\[\\mathbf{v}_k=(-u_1)\\mathbf{v}_1+\\dots+(-u_{k-1})\\mathbf{v}_{k-1}.\\] Thus, the \\(k\\)-th column of the RREF exactly shows the coefficients of how it can be written as a linear combination of the preceding vectors.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2 (T28) How many pivot columns must a \\(5 \\times 7\\) matrix have if its columns span \\(\\mathbb{R}^5\\)?\n\n\n\n\n\n\n\n\n\n\nNoteSolution (Solution to Exercise 2)\n\n\n\n\n\n\nSolution 2 (Solution to Exercise 2). That the vectors span \\(\\mathbb{R}^5\\) means that, for all \\(\\mathbf{b} \\in \\mathbb{R}^5\\), \\(\\mathbf{b}\\) can be expressed as a linear combination of the vectors. That is, the matrix equation \\(A\\mathbf{x}=\\mathbf{b}\\) is consistent. Thus, each of the 5 rows of the REF of A must have a pivot position.\nFor \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(m\\) pivot columns are needed if \\(A\\)’s columns span \\(\\mathbb{R}^m\\). This also states that a minimum set of \\(m\\) vectors can span \\(\\mathbb{R}^m\\) if the vectors are linearly independent.\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. Consider \\(A \\in \\mathbb{R}^{3\\times3}\\) with only two pivot positions. Obviously, the columns of \\(A\\) do not span \\(\\mathbb{R}^3\\) and can only form a plane. But it is also mistaken to say the columns span \\(\\mathbb{R}^2\\) because the vectors are in \\(\\mathbb{R}^3\\).",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "1.7 Linear Independence"
    ]
  },
  {
    "objectID": "part1/chap1.7.html#random-thoughts-about-the-characterization-thm-thm1-7-7",
    "href": "part1/chap1.7.html#random-thoughts-about-the-characterization-thm-thm1-7-7",
    "title": "1.7 Linear Independence",
    "section": "Random Thoughts About the Characterization (Theorem 1)",
    "text": "Random Thoughts About the Characterization (Theorem 1)\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. The textbook warns that Theorem 1 does not guarantee that every vector in a linearly dependent set is a linear combination of the other vectors.\nThe simplest example of this is a zero vector. Consider a linearly independent set \\(S=\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\). Let \\(S'=S \\cup \\{\\mathbf{0}\\}\\), then \\(S'\\) is a linearly dependent set (Theorem 9). Only \\(\\mathbf{0}\\) in \\(S'\\) is a linear combination of the other vectors (\\(0\\mathbf{v}_1+\\dots+0\\mathbf{v}_n\\)) because, for all \\(1 \\leq k \\leq n\\), adding a zero vector doesn’t change the span of the other vectors. Thus, \\(\\mathbf{v}_k \\notin \\text{span}\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_{k-1},\\mathbf{v}_{k+1},\\dots,\\mathbf{v}_n,\\mathbf{0}\\}\\). Similarly, we can add, rather than a zero vector, \\(t\\mathbf{v}_k\\) to \\(S\\), where \\(t \\neq 0\\) and \\(1 \\leq k \\leq n\\). Denote this new set as \\(S'=S \\cup \\{t\\mathbf{v}_k\\}\\). Only the newly added vector, \\(t\\mathbf{v}_k\\), and \\(\\mathbf{v}_k\\) are a linear combination of the other vectors: \\(t\\mathbf{v}_k=t\\cdot\\mathbf{v}_k\\) and \\(\\mathbf{v}_k=\\frac{1}{t}\\cdot t\\mathbf{v}_k\\). Moreover, these are also the only ways by which they can be written as a linear combination of the other vectors.\nThen, what if we add a linear combination of some of the vectors in \\(S\\). Formally, let \\(V=\\{\\mathbf{v}_{i_1},\\dots,\\mathbf{v}_{i_m}\\} \\subseteq S\\). Consider the vector \\(\\mathbf{w}=c_1\\mathbf{v}_{i_1}+\\dots+c_m\\mathbf{v}_{i_m}\\), where \\(c_1 \\cdots c_m \\neq 0\\). \\(S'=S \\cup \\{\\mathbf{w}\\}\\) is linearly dependent. Let \\(V'=V \\cup \\{\\mathbf{w}\\}\\). Since \\(\\mathbf{c}\\) is non-zero, for all \\(\\mathbf{u}\\in V'\\), \\(\\mathbf{u}\\) can be expressed as a unique linear combination of the other vectors in \\(V'\\). The case of the vectors not in \\(V'\\) is the same. They cannot be written as a linear combination of the other vectors because adding a vector already in the span doesn’t change the span.\nLet’s consider a general case in Proposition 1.\n\n\n\n\n\n\n\n\n\n\n\nProposition 1 Suppose \\(S=\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) is an arbitrary set of non-zero vectors in \\(\\mathbb{R}^m\\). It is always possible to partition \\(S\\) into \\(p\\) non-empty subsets, \\(V_1,\\dots,V_p\\), such that, for every \\(|V_k|&gt;1\\) and \\(\\mathbf{v}_i\\in V_k\\), \\(\\mathbf{v}_i\\in\\text{span}(V_k\\setminus \\{\\mathbf{v}_i\\})\\). In addition, if a vector, \\(\\mathbf{v}_i\\in V_k\\), is in \\(\\text{span}(S\\setminus\\{\\mathbf{v}_i\\})\\), it can only be written as a linear combination of the other vectors in \\(V_k\\).\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof of Proposition 1)\n\n\n\n\n\n\nProof (Proof of Proposition 1). The description definitely complicates the thing a lot. Observe that if two subsets with sizes greater than one, \\(V_i,V_j\\), satisfy that every vector in a subset can only be written as a linear combination using the other vectors in the subset, \\(V_i \\cup V_j\\) also meets the requirement.\nThus, the simplest (not unique) partition is making each \\(\\mathbf{v}_i\\notin\\text{span}(S\\setminus\\{\\mathbf{v}_i\\})\\) a single subset. Other vectors that can be represented as a linear combination are put in a same set.\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. What this simplest construction says is that, except for the vectors that are not a linear combination of the other vectors, the remainders are. Pretty naive…\nHowever, this is not what the author was originally thinking. The initial idea was about the finest partition, in which case, for each subset, it is impossible to split it into multiple subsets while satisfying the condition.",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "1.7 Linear Independence"
    ]
  },
  {
    "objectID": "part1/chap1.8.html",
    "href": "part1/chap1.8.html",
    "title": "1.8 Introduction to Linear Transformations",
    "section": "",
    "text": "TipSummary of the Chapter\n\n\n\n\nA transformation \\(T\\) (not necessarily a linear transformation) that maps a vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\) to a vector \\(T(\\mathbf{x})\\in\\mathbb{R}^m\\) is denoted as \\(T: \\mathbb{R}^n\\to\\mathbb{R}^m\\). The \\(\\mathbb{R}^n\\) is called the domain, and the \\(\\mathbb{R}^m\\) is called the codomain.\nNote that although the range of \\(T\\), \\(\\text{Range}(T)=\\{T(\\mathbf{x})\\mid\\mathbf{x}\\in\\mathbb{R}^n\\}\\), may not cover the entire codomain, the dimensionality is the same: \\(\\text{Range}(T)\\subseteq \\mathbb{R}^m\\).\nA matrix transformation is denoted as \\(T(\\mathbf{x})=A\\mathbf{x}\\) or \\(\\mathbf{x}\\mapsto A\\mathbf{x}\\). Suppose \\(A\\) is an \\(m\\times n\\) matrix, then the domain is \\(\\mathbb{R}^n\\), and the codomain is \\(\\mathbb{R}^m\\).\nA linear transformation, \\(T\\), satisfies the following conditions for all \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(c\\):\n\n\\(T(\\mathbf{u}+\\mathbf{v})=T(\\mathbf{u})+T(\\mathbf{v})\\);\n\\(T(c\\mathbf{u})=c T(\\mathbf{u})\\).\n\nThe above two conditions are equivalent to \\(T(c\\mathbf{u}+d\\mathbf{v})=c T(\\mathbf{u})+d T(\\mathbf{v})\\).\nIf \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a linear transformation, \\(T(\\mathbf{0})=\\mathbf{0}\\), but the zero vector on the left hand side is in \\(\\mathbb{R}^n\\), while the one on the right hand side is in \\(\\mathbb{R}^m\\). This characteristic can be proved by substituting \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) with zero vectors and using either of the two conditions.\n\n\n\n\n\n\n\n\n\nTipA Simple Interaction\n\n\n\n\n  \n    \n      Enter a \\(3 \\times 2\\) matrix:\n    \n    \n      \n      \n      \n      \n      \n      \n    \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1 (Example 1.c) Let \\(A=\\begin{bmatrix}1 & -3 \\\\ 3 & 5 \\\\ -1 & 7\\end{bmatrix}, \\mathbf{b}=\\begin{bmatrix}3 \\\\ 2 \\\\ -5\\end{bmatrix}\\), and define a transformation \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^3\\) by \\(T(\\mathbf{x})=A\\mathbf{x}\\). Is there more than one \\(\\mathbf{x}\\) whose image under \\(T\\) is \\(\\mathbf{b}\\)?\n\n\n\n\n\n\n\n\n\n\nNoteSolution (Solution to Exercise 1)\n\n\n\n\n\n\nSolution (Solution to Exercise 1). \\[\\begin{bmatrix}1 & -3 & 3 \\\\ 3 & 5 & 2 \\\\ -1 & 7 & -5\\end{bmatrix} \\sim \\begin{bmatrix}1 & -3 & 3 \\\\ 0 & 1 & -.5 \\\\ 0 & 0 & 0\\end{bmatrix}\\]\nSince there are no free variables (the last column is augmented), there is exactly one \\(x\\) such that \\(T(\\mathbf{x})=\\mathbf{b}\\).\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. Having a row of zeros does not imply that the equation has multiple solutions. It all depends on free variables—namely, the coefficient columns.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2 (T19) Let \\(\\mathbf{e}_1=\\begin{bmatrix}1\\\\ 0\\end{bmatrix}\\), \\(\\mathbf{e}_2=\\begin{bmatrix}0\\\\ 1\\end{bmatrix}\\), \\(\\mathbf{y}_1=\\begin{bmatrix}2\\\\ 5\\end{bmatrix}\\), and \\(\\mathbf{y}_2=\\begin{bmatrix}-1\\\\ 6\\end{bmatrix}\\), and let \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) be a linear transformation that maps \\(\\mathbf{e}_1\\) into \\(\\mathbf{y}_1\\) and maps \\(\\mathbf{e}_2\\) into \\(\\mathbf{y}_2\\). Find the images of \\(\\begin{bmatrix}5\\\\ -3\\end{bmatrix}\\) and \\(\\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolution (Solution to Exercise 2)\n\n\n\n\n\n\nSolution (Solution to Exercise 2). Though it’s possible to solve the transformation matrix because all are in \\(\\mathbb{R}^2\\), the forms of \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) make us easy to represent any \\(\\mathbb{R}^2\\) vectors using \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\).\n\\[\\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}=x_1\\mathbf{e}_1+x_2\\mathbf{e}_2\\]\nThus,\n\\[\\begin{align*}\nT(\\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}) &= T(x_1\\mathbf{e}_1+x_2\\mathbf{e}_2) \\\\\n&= x_1 T(\\mathbf{e}_1) + x_2 T(\\mathbf{e}_2) \\\\\n&= x_1\\mathbf{y}_1 + x_2\\mathbf{y}_2\n\\end{align*}\\]\nA more general form will be discussed in the next chapter.\n\n\n\n\n\n\n\n\n\n\n\nExercise 3 (T25) Given \\(\\mathbf{v}\\neq\\mathbf{0}\\) and \\(\\mathbf{p}\\) in \\(\\mathbb{R}^n\\), the line through \\(\\mathbf{p}\\) in the direction of \\(\\mathbf{v}\\) has the parametric equation \\(\\mathbf{x} = \\mathbf{p} + t\\mathbf{v}\\). Show that a linear transformation \\(T: \\mathbb{R}^n \\to \\mathbb{R}^n\\) maps this line onto another line or onto a single point (a degenerate line).\n\n\n\n\n\n\n\n\n\n\nNoteSolution (Solution to Exercise 3)\n\n\n\n\n\n\nSolution (Solution to Exercise 3). Two points to notice at first:\n\nThis is mentioned not because it could cause misunderstanding, but because it clarifies the meaning of vectors. The textbook is rigorously phrasing the problem as “the line through \\(\\mathbf{p}\\) in the direction of \\(\\mathbf{v}\\)”;\nThe equation is a parametric equation, so all vectors and lines in this problem are in \\(\\mathbb{R}^n\\). Do not confuse this with \\(y=a+bx\\), though the line equation can also be written as \\(\\begin{bmatrix}x\\\\ y\\end{bmatrix}=\\begin{bmatrix}0\\\\ a\\end{bmatrix} + x\\begin{bmatrix}1\\\\ b\\end{bmatrix}\\).\n\nA paraphrase of the problem would be “does applying a linear transformation to this line always yield another line, or possibly a single point?”\n\\[T(\\mathbf{x})=T(\\mathbf{p}+t\\mathbf{v})\\] According to the definition of linear transformations, \\[T(\\mathbf{p}+t\\mathbf{v})=T(\\mathbf{p})+t T(\\mathbf{v}).\\]\nSince \\(T:\\mathbb{R}^n\\to\\mathbb{R}^n\\) maps a vector in \\(\\mathbb{R}^n\\) to another vector in \\(\\mathbb{R}^n\\), \\(T(\\mathbf{p})\\) and \\(T(\\mathbf{v})\\) are both vectors in \\(\\mathbb{R}^n\\). Thus, the new line can be described as a line through \\(T(\\mathbf{p})\\) in the direction of \\(T(\\mathbf{v})\\).\nWhen \\(T(\\mathbf{v})=\\mathbf{0}\\), the line will be mapped onto the single point at \\(T(\\mathbf{p})\\).\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. We now know that applying a linear transformation on a line generates a new line or a point (a degenerate line). Quite linear, right?\n\n\n\n\n\n\n\n\n\n\nTipIs every linear transformation a matrix transformation?\n\n\n\nWe know that every matrix transformation is a linear transformation since \\(A(\\mathbf{u}+\\mathbf{v})=A\\mathbf{u}+A\\mathbf{v}\\) and \\(A(c\\mathbf{u})=c(A\\mathbf{u})\\).\n\nIf we are working in finite-dimensional vector spacese with chosen bases (e.g. \\(\\mathbb{R}^n\\to\\mathbb{R}^m\\)), then yes — every linear transformation is a matrix transformation. The matrix depends on the choice of basis. (We will also learn this in the next chapter.)\nIf we are working in infinite-dimensional vector spaces (like function spaces), then not every linear transformation corresponds to a finite matrix. For example, the derivative operator \\(D: C^\\infty (\\mathbb{R})\\to C^\\infty(\\mathbb{R}),D(f)=f'\\), is linear but cannot be represented by a finite matrix.\n— ChatGPT",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "1.8 Introduction to Linear Transformations"
    ]
  },
  {
    "objectID": "part1/chap1.9.html",
    "href": "part1/chap1.9.html",
    "title": "1.9 The Matrix of a Linear Transformation",
    "section": "",
    "text": "Theorem 1 (Theorem 10) Let \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) be a linear transformation. Then there exists a unique matrix \\(A\\) such that \\[T(\\mathbf{x})=A\\mathbf{x}\\quad\\text{for all }\\mathbf{x}\\text{ in }\\mathbb{R}^n\\] In fact, \\(A\\) is the \\(m\\times n\\) matrix whose \\(j\\) th column is the vector \\(T(\\mathbf{e}_j)\\), where \\(\\mathbf{e}_j\\) is the \\(j\\) th column of the identity matrix in \\(\\mathbb{R}^n\\): \\[A=\\begin{bmatrix}T(\\mathbf{e}_1) & \\cdots & T(\\mathbf{e}_n)\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof of Theorem 1)\n\n\n\n\n\n\nProof (Proof of Theorem 1). The theorem says that every \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) can be represented as a matrix transformation, but we just said not all linear transformations can be represented by a matrix. However, there’s a condition in this theorem. That is, \\(T\\) maps a vector in \\(\\mathbb{R}^n\\), a finite vector space, to a vector in \\(\\mathbb{R}^m\\), also a finite vector space.\nThe proof of the existence is on both the textbook and the previous chapter in this book (a partial proof), so we prove only the uniqueness here.\nIf the matrix \\(A\\) isn’t unique, let \\(B=\\begin{bmatrix}\\mathbf{b}_1 & \\cdots & \\mathbf{b}_n\\end{bmatrix}\\neq A\\) be another matrix.\n\\[T(\\mathbf{e}_j)=B\\mathbf{e}_j=0\\mathbf{b}_1+\\dots+0\\mathbf{b}_{j-1}+\\mathbf{b}_j+0\\mathbf{b}_{j+1}+\\dots+0\\mathbf{b}_n=\\mathbf{b}_j\\]\nHowever, we know that the \\(j\\) th column of \\(A\\) is also \\(T(\\mathbf{e}_j)\\). Thus, for every \\(1\\leq j\\leq n\\), the \\(j\\) th columns of \\(A\\) and \\(B\\) are equal, so \\(A=B\\).\n\n\n\n\n\n\n\n\n\n\nTipIf \\(\\mathbf{e}_1,\\dots,\\mathbf{e}_n\\) are any linear independent set of vectors\n\n\n\nSuppose we need to find \\(T(\\mathbf{x})\\). Since \\(\\mathbf{e}_1,\\dots,\\mathbf{e}_n\\) are linear independent, \\(\\begin{bmatrix}\\mathbf{e}_1 & \\cdots & \\mathbf{e}_n\\end{bmatrix}\\) has a pivot position at each row and each column. The equation \\(\\begin{bmatrix}\\mathbf{e}_1 & \\cdots & \\mathbf{e}_n\\end{bmatrix}\\mathbf{t}=\\mathbf{x}\\) has a unique solution. \\[\\mathbf{t}=\\begin{bmatrix}\\mathbf{e}_1 & \\cdots & \\mathbf{e}_n\\end{bmatrix}^{-1} \\mathbf{x}\\] \\[\\begin{align*}\nT(\\mathbf{x})&=T(t_1\\mathbf{e}_1+\\dots+t_n\\mathbf{e}_n) \\\\\n&=t_1 T(\\mathbf{e}_1)+\\dots+t_n T(\\mathbf{e}_n) \\\\\n&=\\begin{bmatrix}T(\\mathbf{e}_1) & \\cdots & T(\\mathbf{e}_n)\\end{bmatrix}\\mathbf{t} \\\\\n&=\\left(\\begin{bmatrix}T(\\mathbf{e}_1) & \\cdots & T(\\mathbf{e}_n)\\end{bmatrix}\\begin{bmatrix}\\mathbf{e}_1 & \\cdots & \\mathbf{e}_n\\end{bmatrix}^{-1}\\right)\\mathbf{x}\n\\end{align*}\\]",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "1.9 The Matrix of a Linear Transformation"
    ]
  },
  {
    "objectID": "part1/qa.html",
    "href": "part1/qa.html",
    "title": "Questions and Answers",
    "section": "",
    "text": "Questions from Others",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "Questions and Answers"
    ]
  },
  {
    "objectID": "part1/qa.html#questions-from-others",
    "href": "part1/qa.html#questions-from-others",
    "title": "Questions and Answers",
    "section": "",
    "text": "TipWhat are the relationships between linear equations, vectors, matrices, and linear combinations?\n\n\n\nThis explanation is based on how the textbook brings up these concepts.\nAt first, the textbook introduces matrices using linear equations by writing the coefficients line by line. Therefore, a matrix can be a way of representing a system of linear equations. Then, an alternative way to represent the same system of linear equations, vector equations, are introduced. As a recap, a vector equation is defined as follows: \\[x_1\\mathbf{v}_1+\\dots+x_n\\mathbf{v}_n=\\mathbf{b}.\\] To be specific, the vectors discussed so far are column vectors. Subsequently, the linear combination of vectors are defined as \\(c_1\\mathbf{v}_1+\\dots+c_n\\mathbf{v}_n\\). All linear combinations of the set of vectors are denoted as \\(\\text{span}\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}=\\{c_1\\mathbf{v}_1+\\dots+c_n\\mathbf{v_n}\\mid c_1,\\dots,c_n\\in \\mathbb{R}\\}\\).\nAfterward, the multiplication of a matrix and a column vector is defined: \\[\\begin{bmatrix}\\mathbf{v}_1 & \\cdots & \\mathbf{v}_n\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix}=x_1\\mathbf{v}_1+\\dots+x_n\\mathbf{v}_n.\\] Observably, the result of the multiplication is a linear combination of \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\) using \\(x_1,\\dots,x_n\\) as the coefficients. Suppose \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\in\\mathbb{R}^m\\), then \\(\\begin{bmatrix}\\mathbf{v}_1 & \\cdots & \\mathbf{v}_n\\end{bmatrix}\\) is an \\(m\\times n\\) matrix. Thus, the previously introduced vector equation can be written as \\(A\\mathbf{x}=\\mathbf{b}\\), where \\(A=\\begin{bmatrix}\\mathbf{v}_1 & \\cdots & \\mathbf{v}_n\\end{bmatrix}\\) and \\(\\mathbf{x}=\\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix}\\).\nViewing the coefficients \\(c_1,\\dots,c_n\\) in linear combinations as the unknowns, finding a set of coefficients whose corresponding linear combination is \\(\\mathbf{b}\\) is equivalent to solving the vector equation \\(c_1\\mathbf{v}_1+\\dots+c_n\\mathbf{v}_n=\\mathbf{b}\\), or \\(\\begin{bmatrix}\\mathbf{v}_1 & \\cdots & \\mathbf{v}_n\\end{bmatrix}\\begin{bmatrix}c_1\\\\\\vdots\\\\c_n\\end{bmatrix}=\\mathbf{b}\\), which all tie back to a set of linear equations. Later, the concept of linear independence is defined: \\(A\\mathbf{x}=\\mathbf{0}\\) has only the trivial solution, which can be judged using the echelon forms of a matrix.",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "Questions and Answers"
    ]
  },
  {
    "objectID": "part1/qa.html#my-stupid-questions",
    "href": "part1/qa.html#my-stupid-questions",
    "title": "Questions and Answers",
    "section": "My Stupid Questions",
    "text": "My Stupid Questions\n\n\n\n\n\n\nTipWhat does it mean to transform \\(\\mathbf{e}_2\\) to \\(\\mathbf{e}_2-2\\mathbf{e}_1\\) …\n\n\n\nOut of unknown reason, the author first interpreted this as mapping \\(x_2\\) to \\(x_2-2x_1\\). This is incorrect because \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) are vectors, not simple numbers. The result of a transformation \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) that maps \\(\\mathbf{e}_2\\) to \\(\\mathbf{e}_2-2\\mathbf{e}_1\\) and leaves \\(\\mathbf{e}_1\\) unchanged is very different. The transformation actually changes \\(x_1\\) to \\(x_1-2x_2\\) while leaving \\(x_2\\) unchanged. Each \\(\\mathbf{x}\\) in the domain is a linear combination of the basis, and the image is also a linear combination of the images of the basis. Moreover, it wouldn’t make sense to interpret the problem as mapping \\(x_2\\) to \\(x_2-2x_1\\) since such transformation cannot change the dimensionality.",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "Questions and Answers"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Lay, David C, Steven R Lay, and Judith McDonald. 2016. Linear\nAlgebra and Its Applications. 5th ed. Pearson.",
    "crumbs": [
      "References"
    ]
  }
]