[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Some Notes on Linear Algebra",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nTip\n\n\n\nRecommended Reading:\n\nPart 4\n\n4.6 Rank - Exploration on Orthogonality\nMiscellaneous - Solving for Bases Using Column Operations\n\n3.1 Introduction to Determinants - Proof of Theorems\n1.7 Linear Independence - Random Thoughts on Its Characterization\n\nNot Recommended Reading:\n\n2.5 Matrix Factorizations (Reason: flawed proofs)\n\nPending:\n\n3.2 [R] Number Theory? - Quotient Spaces\n\n\n\nThis website contains notes, including proofs, random thoughts, and explorations, when I am taking a linear algebra course in grade 12.\nThe textbook used in the course, Linear Algebra and Its Applications 5th Edition (Lay, Lay, and McDonald 2016), serves as the basis of this book.\n\n\n\n\n\n\nTipExplanation of notation in titles\n\n\n\n\n[R]: Random thoughts and explorations\n\n\n\n\\[\\begin{align}\\end{align}\\]\n\n\n\n\nLay, David C, Steven R Lay, and Judith McDonald. 2016. Linear Algebra and Its Applications. 5th ed. Pearson.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part1/chap1.7.html",
    "href": "part1/chap1.7.html",
    "title": "1.7 Linear Independence",
    "section": "",
    "text": "Definition and Characterization",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "1.7 Linear Independence"
    ]
  },
  {
    "objectID": "part1/chap1.7.html#definition-and-characterization",
    "href": "part1/chap1.7.html#definition-and-characterization",
    "title": "1.7 Linear Independence",
    "section": "",
    "text": "NoteDefinition (Linear Independence)\n\n\n\n\nDefinition 1 (Linear Independence) An indexed set of vectors \\(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) in \\(\\mathbb{R}^m\\) is said to be linearly independent if the vector equation \\[x_1\\mathbf{v}_1+x_2\\mathbf{v}_2+\\dots+x_n\\mathbf{v}_n=\\mathbf{0}\\] has only the trivial solution (\\(\\mathbf{x}=\\mathbf{0}\\)). Otherwise, it is said to be linearly dependent.\n\n\n\n\n\n\n\n\n\n\nTheorem 1 (Theorem 7: Characterization of Linearly Dependent Sets) An indexed set \\(S = \\{\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\}\\) of two or more vectors is linearly dependent if and only if at least one of the vectors in \\(S\\) is a linear combination of the others. In fact, if \\(S\\) is linearly dependent and \\(\\mathbf{v}_1 \\neq \\mathbf{0}\\), then some \\(\\mathbf{v}_j\\) (with \\(j &gt; 1\\)) is a linear combination of the preceding vectors, \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_{j-1}\\).\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof of Theorem 1)\n\n\n\n\n\n\nProof (Proof of Theorem 1). Let “in fact” separate the theorem into two parts. Consider the first part.\n\nIf there is a vector that is a linear combination of the other vectors, \\[\\mathbf{v}_i=c_1\\mathbf{v}_1+\\dots+c_{i-1}\\mathbf{v}_{i-1}+c_{i+1}\\mathbf{v}_{i+1}+\\dots+c_n\\mathbf{v}_n,\\] move the terms into one side: \\[c_1\\mathbf{v}_1+\\dots+c_{i-1}\\mathbf{v}_{i-1}+(-1)\\mathbf{v}_i+c_{i+1}\\mathbf{v}_{i+1}+\\dots+c_n\\mathbf{v}_n=\\mathbf{0}.\\] Thus, there is a nontrivial solution for \\(\\mathbf{c}\\), where \\(c_i=-1\\), so the set is linearly dependent.\nIf the set is linearly dependent, the vector equation \\(c_1\\mathbf{v}_1+\\dots+c_n\\mathbf{v}_n=\\mathbf{0}\\) has nontrivial solution of \\(\\mathbf{c}\\). There exists an index \\(i\\) where \\(c_i \\neq 0\\). Move the \\(i\\)-th term to the left hand side and the other terms to the right hand side: \\[-c_i\\mathbf{v}_i=c_1\\mathbf{v}_1+\\dots+c_{i-1}\\mathbf{v}_{i-1}+c_{i+1}\\mathbf{v}_{i+1}+\\dots+c_n\\mathbf{v}_n.\\] Divide both sides by \\(-c_i\\): \\[\\mathbf{v}_i=\\frac{c_1}{-c_i}\\mathbf{v}_1+\\dots+\\frac{c_{i-1}}{-c_i}\\mathbf{v}_{i-1}+\\frac{c_{i+1}}{-c_i}\\mathbf{v}_{i+1}+\\dots+\\frac{c_n}{-c_i}\\mathbf{v}_n.\\] Hence, \\(\\mathbf{v}_i\\) is a linear combination of the other vectors in the set.\n\nWe have now proved the first part of the theorem. Let’s prove the second part by induction and contradiction.\nIf \\(\\mathbf{v}_j = \\mathbf{0}\\) (\\(1 &lt; j \\leq n\\)), \\(\\mathbf{v}_j\\) can be expressed as \\(0\\cdot\\mathbf{v}_1+\\dots+0\\cdot\\mathbf{v}_{j-1}\\), so we only consider situations that, for all \\(1 \\leq j \\leq n\\), \\(\\mathbf{v}_j\\neq\\mathbf{0}\\).\nLet \\(A_k=\\begin{bmatrix} \\mathbf{v}_1 & \\cdots & \\mathbf{v}_k \\end{bmatrix}\\), where \\(1 \\leq k \\leq n\\), so \\(A \\in \\mathbb{R}^{m \\times k}\\).\nThe reduced row echelon form (RREF) of \\(A_1\\) is \\(\\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\). If \\(\\mathbf{v}_2\\) is not a linear combination of \\(\\mathbf{v}_1\\), \\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}\\) is linearly independent. Thus, the RREF of \\(A_2\\) is \\(\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ \\vdots & \\vdots \\\\ 0 & 0 \\end{bmatrix}\\). Similarly, the RREF of \\(A_3\\) needs to be \\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ \\vdots & \\vdots & \\vdots \\\\ 0 & 0 & 0 \\end{bmatrix}\\), since if there is no pivot position in the third column, the solution of \\(A_3\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\mathbf{0}\\) can be written as \\(\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=x_3\\begin{bmatrix}u_1\\\\u_2\\\\1\\end{bmatrix}\\), where \\(x_3,u_1,u_2\\in\\mathbb{R}\\). Thus, the RREF of \\(A_k\\) must be in the form of \\(\\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\). When \\(k=n\\), the matrix equation \\(A_n\\mathbf{x}=\\mathbf{0}\\) has only the trivial solution because every column has a pivot position. Hence, \\(S\\) is linearly independent, contradicting the stated condition that \\(S\\) is linearly dependent.\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. When the author first learned about linear independence, the characteristic (Theorem 1), rather than the definition (Definition 1), was taught first. That is, if none of the vectors in a set are a linear combination of the other vectors, the set is linearly independent. This characterization may more intuitively illustrates the name “linear independence” than the definition does, as every vector is “independent” of the other vectors.",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "1.7 Linear Independence"
    ]
  },
  {
    "objectID": "part1/chap1.7.html#pivot-columns-and-linear-independence",
    "href": "part1/chap1.7.html#pivot-columns-and-linear-independence",
    "title": "1.7 Linear Independence",
    "section": "Pivot Columns and Linear Independence",
    "text": "Pivot Columns and Linear Independence\n\n\n\n\n\n\n\nExercise 1 (T27) How many pivot columns must a \\(7 \\times 5\\) matrix have if its columns are linearly independent?\n\n\n\n\n\n\n\n\n\n\nNoteSolution (Solution to Exercise 1)\n\n\n\n\n\n\nSolution 1 (Solution to Exercise 1). Recall the definition of linear indenpendence: \\(A\\mathbf{x}=\\mathbf{0}\\) has only the trivial solution (\\(\\mathbf{x}=\\mathbf{0}\\)). There must be 5 pivot columns in \\(A\\)’s row echelon form (REF). Otherwise, the free variable(s) will produce solutions other than \\(\\mathbf{0}\\).\nIf we generalize the situation to \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(n\\) pivot columns are required to avoid free variables. Therefore, the columns of \\(A\\) are necessarily linearly dependent when \\(n &gt; m\\), as there are no enough rows to generate \\(n\\) pivot columns (Theorem 8).\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. What if there are columns without a pivot position?\nLet the \\(k\\)-th column be one of those columns. Then, \\(x_k\\) is a free variable. According to the proof of Theorem 1, \\(\\mathbf{v}_k \\in \\text{span}\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_{k-1}\\}\\). In addition, we can observe the solution of the matrix equation \\(A\\mathbf{x}=\\mathbf{0}\\), \\(\\mathbf{x}=x_k\\mathbf{u}+x_{k_1}\\mathbf{u}_1+\\dots+x_{k_t}\\mathbf{u}_t\\), where \\(\\mathbf{u},\\mathbf{u}_1,\\dots,\\mathbf{u}_t\\) are the vectors obtained from the RREF and \\(x_{k_1},\\dots,x_{k_t}\\) are other free variables. Set \\(x_{k_1},\\dots,x_{k_t}\\) equal to zero: \\[\\mathbf{x}=x_k\\mathbf{u}.\\] Substitute this into the vector equation, \\(x_1\\mathbf{v}_1+\\dots+x_n\\mathbf{v}_n=\\mathbf{0}\\): \\[x_k u_1\\mathbf{v}_1+\\dots+x_k u_k\\mathbf{v}_k+\\dots+x_k u_n\\mathbf{v}_n=\\mathbf{0}.\\] Since \\(x_k\\) is a free variable, we can divide the equation by \\(x_k\\) if \\(x_k \\neq 0\\). Then, separate the \\(k\\)-th term to one side (\\(u_k=1\\)): \\[\\mathbf{v}_k=(-u_1)\\mathbf{v}_1+\\dots+(-u_n)\\mathbf{v}_n.\\] Since the matrix has been simplified to its RREF, \\(u_{k+1},\\dots,u_n\\) all equal zero. Then, \\[\\mathbf{v}_k=(-u_1)\\mathbf{v}_1+\\dots+(-u_{k-1})\\mathbf{v}_{k-1}.\\] Thus, the \\(k\\)-th column of the RREF exactly shows the coefficients of how it can be written as a linear combination of the preceding vectors.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2 (T28) How many pivot columns must a \\(5 \\times 7\\) matrix have if its columns span \\(\\mathbb{R}^5\\)?\n\n\n\n\n\n\n\n\n\n\nNoteSolution (Solution to Exercise 2)\n\n\n\n\n\n\nSolution 2 (Solution to Exercise 2). That the vectors span \\(\\mathbb{R}^5\\) means that, for all \\(\\mathbf{b} \\in \\mathbb{R}^5\\), \\(\\mathbf{b}\\) can be expressed as a linear combination of the vectors. That is, the matrix equation \\(A\\mathbf{x}=\\mathbf{b}\\) is consistent. Thus, each of the 5 rows of the REF of A must have a pivot position.\nFor \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(m\\) pivot columns are needed if \\(A\\)’s columns span \\(\\mathbb{R}^m\\). This also states that a minimum set of \\(m\\) vectors can span \\(\\mathbb{R}^m\\) if the vectors are linearly independent.\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. Consider \\(A \\in \\mathbb{R}^{3\\times3}\\) with only two pivot positions. Obviously, the columns of \\(A\\) do not span \\(\\mathbb{R}^3\\) and can only form a plane. But it is also mistaken to say the columns span \\(\\mathbb{R}^2\\) because the vectors are in \\(\\mathbb{R}^3\\).",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "1.7 Linear Independence"
    ]
  },
  {
    "objectID": "part1/chap1.7.html#random-thoughts-about-the-characterization-thm-thm1-7-7",
    "href": "part1/chap1.7.html#random-thoughts-about-the-characterization-thm-thm1-7-7",
    "title": "1.7 Linear Independence",
    "section": "Random Thoughts About the Characterization (Theorem 1)",
    "text": "Random Thoughts About the Characterization (Theorem 1)\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. The textbook warns that Theorem 1 does not guarantee that every vector in a linearly dependent set is a linear combination of the other vectors.\nThe simplest example of this is a zero vector. Consider a linearly independent set \\(S=\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\). Let \\(S'=S \\cup \\{\\mathbf{0}\\}\\), then \\(S'\\) is a linearly dependent set (Theorem 9). Only \\(\\mathbf{0}\\) in \\(S'\\) is a linear combination of the other vectors (\\(0\\mathbf{v}_1+\\dots+0\\mathbf{v}_n\\)) because, for all \\(1 \\leq k \\leq n\\), adding a zero vector doesn’t change the span of the other vectors. Thus, \\(\\mathbf{v}_k \\notin \\text{span}\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_{k-1},\\mathbf{v}_{k+1},\\dots,\\mathbf{v}_n,\\mathbf{0}\\}\\). Similarly, we can add, rather than a zero vector, \\(t\\mathbf{v}_k\\) to \\(S\\), where \\(t \\neq 0\\) and \\(1 \\leq k \\leq n\\). Denote this new set as \\(S'=S \\cup \\{t\\mathbf{v}_k\\}\\). Only the newly added vector, \\(t\\mathbf{v}_k\\), and \\(\\mathbf{v}_k\\) are a linear combination of the other vectors: \\(t\\mathbf{v}_k=t\\cdot\\mathbf{v}_k\\) and \\(\\mathbf{v}_k=\\frac{1}{t}\\cdot t\\mathbf{v}_k\\). Moreover, these are also the only ways by which they can be written as a linear combination of the other vectors.\nThen, what if we add a linear combination of some of the vectors in \\(S\\). Formally, let \\(V=\\{\\mathbf{v}_{i_1},\\dots,\\mathbf{v}_{i_m}\\} \\subseteq S\\). Consider the vector \\(\\mathbf{w}=c_1\\mathbf{v}_{i_1}+\\dots+c_m\\mathbf{v}_{i_m}\\), where \\(c_1 \\cdots c_m \\neq 0\\). \\(S'=S \\cup \\{\\mathbf{w}\\}\\) is linearly dependent. Let \\(V'=V \\cup \\{\\mathbf{w}\\}\\). Since \\(\\mathbf{c}\\) is non-zero, for all \\(\\mathbf{u}\\in V'\\), \\(\\mathbf{u}\\) can be expressed as a unique linear combination of the other vectors in \\(V'\\). The case of the vectors not in \\(V'\\) is the same. They cannot be written as a linear combination of the other vectors because adding a vector already in the span doesn’t change the span.\nLet’s consider a general case in Proposition 1.\n\n\n\n\n\n\n\n\n\n\n\nProposition 1 Suppose \\(S=\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) is an arbitrary set of non-zero vectors in \\(\\mathbb{R}^m\\). It is always possible to partition \\(S\\) into \\(p\\) non-empty subsets, \\(V_1,\\dots,V_p\\), such that, for every \\(|V_k|&gt;1\\) and \\(\\mathbf{v}_i\\in V_k\\), \\(\\mathbf{v}_i\\in\\text{span}(V_k\\setminus \\{\\mathbf{v}_i\\})\\). In addition, if a vector, \\(\\mathbf{v}_i\\in V_k\\), is in \\(\\text{span}(S\\setminus\\{\\mathbf{v}_i\\})\\), it can only be written as a linear combination of the other vectors in \\(V_k\\).\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof of Proposition 1)\n\n\n\n\n\n\nProof (Proof of Proposition 1). The description definitely complicates the thing a lot. Observe that if two subsets with sizes greater than one, \\(V_i,V_j\\), satisfy that every vector in a subset can only be written as a linear combination using the other vectors in the subset, \\(V_i \\cup V_j\\) also meets the requirement.\nThus, the simplest (not unique) partition is making each \\(\\mathbf{v}_i\\notin\\text{span}(S\\setminus\\{\\mathbf{v}_i\\})\\) a single subset. Other vectors that can be represented as a linear combination are put in a same set.\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. What this simplest construction says is that, except for the vectors that are not a linear combination of the other vectors, the remainders are. Pretty naive…\nHowever, this is not what the author was originally thinking. The initial idea was about the finest partition, in which case, for each subset, it is impossible to split it into multiple subsets while satisfying the condition.",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "1.7 Linear Independence"
    ]
  },
  {
    "objectID": "part1/chap1.8.html",
    "href": "part1/chap1.8.html",
    "title": "1.8 Introduction to Linear Transformations",
    "section": "",
    "text": "TipSummary of the Chapter\n\n\n\n\nA transformation \\(T\\) (not necessarily a linear transformation) that maps a vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\) to a vector \\(T(\\mathbf{x})\\in\\mathbb{R}^m\\) is denoted as \\(T: \\mathbb{R}^n\\to\\mathbb{R}^m\\). The \\(\\mathbb{R}^n\\) is called the domain, and the \\(\\mathbb{R}^m\\) is called the codomain.\nNote that although the range of \\(T\\), \\(\\text{Range}(T)=\\{T(\\mathbf{x})\\mid\\mathbf{x}\\in\\mathbb{R}^n\\}\\), may not cover the entire codomain, the dimensionality is the same: \\(\\text{Range}(T)\\subseteq \\mathbb{R}^m\\).\nA matrix transformation is denoted as \\(T(\\mathbf{x})=A\\mathbf{x}\\) or \\(\\mathbf{x}\\mapsto A\\mathbf{x}\\). Suppose \\(A\\) is an \\(m\\times n\\) matrix, then the domain is \\(\\mathbb{R}^n\\), and the codomain is \\(\\mathbb{R}^m\\).\nA linear transformation, \\(T\\), satisfies the following conditions for all \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(c\\):\n\n\\(T(\\mathbf{u}+\\mathbf{v})=T(\\mathbf{u})+T(\\mathbf{v})\\);\n\\(T(c\\mathbf{u})=c T(\\mathbf{u})\\).\n\nThe above two conditions are equivalent to \\(T(c\\mathbf{u}+d\\mathbf{v})=c T(\\mathbf{u})+d T(\\mathbf{v})\\).\nIf \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a linear transformation, \\(T(\\mathbf{0})=\\mathbf{0}\\), but the zero vector on the left hand side is in \\(\\mathbb{R}^n\\), while the one on the right hand side is in \\(\\mathbb{R}^m\\). This characteristic can be proved by substituting \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) with zero vectors and using either of the two conditions.\n\n\n\n\n\n\n\n\n\nTipA Simple Interaction\n\n\n\n\n  \n    \n      Enter a \\(3 \\times 2\\) matrix:\n    \n    \n      \n      \n      \n      \n      \n      \n    \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1 (Example 1.c) Let \\(A=\\begin{bmatrix}1 & -3 \\\\ 3 & 5 \\\\ -1 & 7\\end{bmatrix}, \\mathbf{b}=\\begin{bmatrix}3 \\\\ 2 \\\\ -5\\end{bmatrix}\\), and define a transformation \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^3\\) by \\(T(\\mathbf{x})=A\\mathbf{x}\\). Is there more than one \\(\\mathbf{x}\\) whose image under \\(T\\) is \\(\\mathbf{b}\\)?\n\n\n\n\n\n\n\n\n\n\nNoteSolution (Solution to Exercise 1)\n\n\n\n\n\n\nSolution (Solution to Exercise 1). \\[\\begin{bmatrix}1 & -3 & 3 \\\\ 3 & 5 & 2 \\\\ -1 & 7 & -5\\end{bmatrix} \\sim \\begin{bmatrix}1 & -3 & 3 \\\\ 0 & 1 & -.5 \\\\ 0 & 0 & 0\\end{bmatrix}\\]\nSince there are no free variables (the last column is augmented), there is exactly one \\(x\\) such that \\(T(\\mathbf{x})=\\mathbf{b}\\).\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. Having a row of zeros does not imply that the equation has multiple solutions. It all depends on free variables—namely, the coefficient columns.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2 (T19) Let \\(\\mathbf{e}_1=\\begin{bmatrix}1\\\\ 0\\end{bmatrix}\\), \\(\\mathbf{e}_2=\\begin{bmatrix}0\\\\ 1\\end{bmatrix}\\), \\(\\mathbf{y}_1=\\begin{bmatrix}2\\\\ 5\\end{bmatrix}\\), and \\(\\mathbf{y}_2=\\begin{bmatrix}-1\\\\ 6\\end{bmatrix}\\), and let \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) be a linear transformation that maps \\(\\mathbf{e}_1\\) into \\(\\mathbf{y}_1\\) and maps \\(\\mathbf{e}_2\\) into \\(\\mathbf{y}_2\\). Find the images of \\(\\begin{bmatrix}5\\\\ -3\\end{bmatrix}\\) and \\(\\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolution (Solution to Exercise 2)\n\n\n\n\n\n\nSolution (Solution to Exercise 2). Though it’s possible to solve the transformation matrix because all are in \\(\\mathbb{R}^2\\), the forms of \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) make us easy to represent any \\(\\mathbb{R}^2\\) vectors using \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\).\n\\[\\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}=x_1\\mathbf{e}_1+x_2\\mathbf{e}_2\\]\nThus,\n\\[\\begin{align*}\nT(\\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}) &= T(x_1\\mathbf{e}_1+x_2\\mathbf{e}_2) \\\\\n&= x_1 T(\\mathbf{e}_1) + x_2 T(\\mathbf{e}_2) \\\\\n&= x_1\\mathbf{y}_1 + x_2\\mathbf{y}_2\n\\end{align*}\\]\nA more general form will be discussed in the next chapter.\n\n\n\n\n\n\n\n\n\n\n\nExercise 3 (T25) Given \\(\\mathbf{v}\\neq\\mathbf{0}\\) and \\(\\mathbf{p}\\) in \\(\\mathbb{R}^n\\), the line through \\(\\mathbf{p}\\) in the direction of \\(\\mathbf{v}\\) has the parametric equation \\(\\mathbf{x} = \\mathbf{p} + t\\mathbf{v}\\). Show that a linear transformation \\(T: \\mathbb{R}^n \\to \\mathbb{R}^n\\) maps this line onto another line or onto a single point (a degenerate line).\n\n\n\n\n\n\n\n\n\n\nNoteSolution (Solution to Exercise 3)\n\n\n\n\n\n\nSolution (Solution to Exercise 3). Two points to notice at first:\n\nThis is mentioned not because it could cause misunderstanding, but because it clarifies the meaning of vectors. The textbook is rigorously phrasing the problem as “the line through \\(\\mathbf{p}\\) in the direction of \\(\\mathbf{v}\\)”;\nThe equation is a parametric equation, so all vectors and lines in this problem are in \\(\\mathbb{R}^n\\). Do not confuse this with \\(y=a+bx\\), though the line equation can also be written as \\(\\begin{bmatrix}x\\\\ y\\end{bmatrix}=\\begin{bmatrix}0\\\\ a\\end{bmatrix} + x\\begin{bmatrix}1\\\\ b\\end{bmatrix}\\).\n\nA paraphrase of the problem would be “does applying a linear transformation to this line always yield another line, or possibly a single point?”\n\\[T(\\mathbf{x})=T(\\mathbf{p}+t\\mathbf{v})\\] According to the definition of linear transformations, \\[T(\\mathbf{p}+t\\mathbf{v})=T(\\mathbf{p})+t T(\\mathbf{v}).\\]\nSince \\(T:\\mathbb{R}^n\\to\\mathbb{R}^n\\) maps a vector in \\(\\mathbb{R}^n\\) to another vector in \\(\\mathbb{R}^n\\), \\(T(\\mathbf{p})\\) and \\(T(\\mathbf{v})\\) are both vectors in \\(\\mathbb{R}^n\\). Thus, the new line can be described as a line through \\(T(\\mathbf{p})\\) in the direction of \\(T(\\mathbf{v})\\).\nWhen \\(T(\\mathbf{v})=\\mathbf{0}\\), the line will be mapped onto the single point at \\(T(\\mathbf{p})\\).\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. We now know that applying a linear transformation on a line generates a new line or a point (a degenerate line). Quite linear, right?\n\n\n\n\n\n\n\n\n\n\nTipIs every linear transformation a matrix transformation?\n\n\n\nWe know that every matrix transformation is a linear transformation since \\(A(\\mathbf{u}+\\mathbf{v})=A\\mathbf{u}+A\\mathbf{v}\\) and \\(A(c\\mathbf{u})=c(A\\mathbf{u})\\).\n\nIf we are working in finite-dimensional vector spacese with chosen bases (e.g. \\(\\mathbb{R}^n\\to\\mathbb{R}^m\\)), then yes — every linear transformation is a matrix transformation. The matrix depends on the choice of basis. (We will also learn this in the next chapter.)\nIf we are working in infinite-dimensional vector spaces (like function spaces), then not every linear transformation corresponds to a finite matrix. For example, the derivative operator \\(D: C^\\infty (\\mathbb{R})\\to C^\\infty(\\mathbb{R}),D(f)=f'\\), is linear but cannot be represented by a finite matrix.\n— ChatGPT",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "1.8 Introduction to Linear Transformations"
    ]
  },
  {
    "objectID": "part1/chap1.9.html",
    "href": "part1/chap1.9.html",
    "title": "1.9 The Matrix of a Linear Transformation",
    "section": "",
    "text": "Theorem 1 (Theorem 10) Let \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) be a linear transformation. Then there exists a unique matrix \\(A\\) such that \\[T(\\mathbf{x})=A\\mathbf{x}\\quad\\text{for all }\\mathbf{x}\\text{ in }\\mathbb{R}^n\\] In fact, \\(A\\) is the \\(m\\times n\\) matrix whose \\(j\\) th column is the vector \\(T(\\mathbf{e}_j)\\), where \\(\\mathbf{e}_j\\) is the \\(j\\) th column of the identity matrix in \\(\\mathbb{R}^n\\): \\[A=\\begin{bmatrix}T(\\mathbf{e}_1) & \\cdots & T(\\mathbf{e}_n)\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof of Theorem 1)\n\n\n\n\n\n\nProof (Proof of Theorem 1). The theorem says that every \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) can be represented as a matrix transformation, but we just said not all linear transformations can be represented by a matrix. However, there’s a condition in this theorem. That is, \\(T\\) maps a vector in \\(\\mathbb{R}^n\\), a finite vector space, to a vector in \\(\\mathbb{R}^m\\), also a finite vector space.\nThe proof of the existence is on both the textbook and the previous chapter in this book (a partial proof), so we prove only the uniqueness here.\nIf the matrix \\(A\\) isn’t unique, let \\(B=\\begin{bmatrix}\\mathbf{b}_1 & \\cdots & \\mathbf{b}_n\\end{bmatrix}\\neq A\\) be another matrix.\n\\[T(\\mathbf{e}_j)=B\\mathbf{e}_j=0\\mathbf{b}_1+\\dots+0\\mathbf{b}_{j-1}+\\mathbf{b}_j+0\\mathbf{b}_{j+1}+\\dots+0\\mathbf{b}_n=\\mathbf{b}_j\\]\nHowever, we know that the \\(j\\) th column of \\(A\\) is also \\(T(\\mathbf{e}_j)\\). Thus, for every \\(1\\leq j\\leq n\\), the \\(j\\) th columns of \\(A\\) and \\(B\\) are equal, so \\(A=B\\).\n\n\n\n\n\n\n\n\n\n\nTipIf \\(\\mathbf{e}_1,\\dots,\\mathbf{e}_n\\) are any linear independent set of vectors\n\n\n\nSuppose we need to find \\(T(\\mathbf{x})\\). Since \\(\\mathbf{e}_1,\\dots,\\mathbf{e}_n\\) are linear independent, \\(\\begin{bmatrix}\\mathbf{e}_1 & \\cdots & \\mathbf{e}_n\\end{bmatrix}\\) has a pivot position at each row and each column. The equation \\(\\begin{bmatrix}\\mathbf{e}_1 & \\cdots & \\mathbf{e}_n\\end{bmatrix}\\mathbf{t}=\\mathbf{x}\\) has a unique solution. \\[\\mathbf{t}=\\begin{bmatrix}\\mathbf{e}_1 & \\cdots & \\mathbf{e}_n\\end{bmatrix}^{-1} \\mathbf{x}\\] \\[\\begin{align*}\nT(\\mathbf{x})&=T(t_1\\mathbf{e}_1+\\dots+t_n\\mathbf{e}_n) \\\\\n&=t_1 T(\\mathbf{e}_1)+\\dots+t_n T(\\mathbf{e}_n) \\\\\n&=\\begin{bmatrix}T(\\mathbf{e}_1) & \\cdots & T(\\mathbf{e}_n)\\end{bmatrix}\\mathbf{t} \\\\\n&=\\left(\\begin{bmatrix}T(\\mathbf{e}_1) & \\cdots & T(\\mathbf{e}_n)\\end{bmatrix}\\begin{bmatrix}\\mathbf{e}_1 & \\cdots & \\mathbf{e}_n\\end{bmatrix}^{-1}\\right)\\mathbf{x}\n\\end{align*}\\]",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "1.9 The Matrix of a Linear Transformation"
    ]
  },
  {
    "objectID": "part1/qa.html",
    "href": "part1/qa.html",
    "title": "Questions and Answers",
    "section": "",
    "text": "Questions from Others",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "Questions and Answers"
    ]
  },
  {
    "objectID": "part1/qa.html#questions-from-others",
    "href": "part1/qa.html#questions-from-others",
    "title": "Questions and Answers",
    "section": "",
    "text": "TipWhat are the relationships between linear equations, vectors, matrices, and linear combinations?\n\n\n\nThis explanation is based on how the textbook brings up these concepts.\nAt first, the textbook introduces matrices using linear equations by writing the coefficients line by line. Therefore, a matrix can be a way of representing a system of linear equations. Then, an alternative way to represent the same system of linear equations, vector equations, are introduced. As a recap, a vector equation is defined as follows: \\[x_1\\mathbf{v}_1+\\dots+x_n\\mathbf{v}_n=\\mathbf{b}.\\] To be specific, the vectors discussed so far are column vectors. Subsequently, the linear combination of vectors are defined as \\(c_1\\mathbf{v}_1+\\dots+c_n\\mathbf{v}_n\\). All linear combinations of the set of vectors are denoted as \\(\\text{span}\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}=\\{c_1\\mathbf{v}_1+\\dots+c_n\\mathbf{v_n}\\mid c_1,\\dots,c_n\\in \\mathbb{R}\\}\\).\nAfterward, the multiplication of a matrix and a column vector is defined: \\[\\begin{bmatrix}\\mathbf{v}_1 & \\cdots & \\mathbf{v}_n\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix}=x_1\\mathbf{v}_1+\\dots+x_n\\mathbf{v}_n.\\] Observably, the result of the multiplication is a linear combination of \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\) using \\(x_1,\\dots,x_n\\) as the coefficients. Suppose \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\in\\mathbb{R}^m\\), then \\(\\begin{bmatrix}\\mathbf{v}_1 & \\cdots & \\mathbf{v}_n\\end{bmatrix}\\) is an \\(m\\times n\\) matrix. Thus, the previously introduced vector equation can be written as \\(A\\mathbf{x}=\\mathbf{b}\\), where \\(A=\\begin{bmatrix}\\mathbf{v}_1 & \\cdots & \\mathbf{v}_n\\end{bmatrix}\\) and \\(\\mathbf{x}=\\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix}\\).\nViewing the coefficients \\(c_1,\\dots,c_n\\) in linear combinations as the unknowns, finding a set of coefficients whose corresponding linear combination is \\(\\mathbf{b}\\) is equivalent to solving the vector equation \\(c_1\\mathbf{v}_1+\\dots+c_n\\mathbf{v}_n=\\mathbf{b}\\), or \\(\\begin{bmatrix}\\mathbf{v}_1 & \\cdots & \\mathbf{v}_n\\end{bmatrix}\\begin{bmatrix}c_1\\\\\\vdots\\\\c_n\\end{bmatrix}=\\mathbf{b}\\), which all tie back to a set of linear equations. Later, the concept of linear independence is defined: \\(A\\mathbf{x}=\\mathbf{0}\\) has only the trivial solution, which can be judged using the echelon forms of a matrix.",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "Questions and Answers"
    ]
  },
  {
    "objectID": "part1/qa.html#my-stupid-questions",
    "href": "part1/qa.html#my-stupid-questions",
    "title": "Questions and Answers",
    "section": "My Stupid Questions",
    "text": "My Stupid Questions\n\n\n\n\n\n\nTipWhat does it mean to transform \\(\\mathbf{e}_2\\) to \\(\\mathbf{e}_2-2\\mathbf{e}_1\\) …\n\n\n\nOut of unknown reason, the author first interpreted this as mapping \\(x_2\\) to \\(x_2-2x_1\\). This is incorrect because \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) are vectors, not simple numbers. The result of a transformation \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) that maps \\(\\mathbf{e}_2\\) to \\(\\mathbf{e}_2-2\\mathbf{e}_1\\) and leaves \\(\\mathbf{e}_1\\) unchanged is very different. The transformation actually changes \\(x_1\\) to \\(x_1-2x_2\\) while leaving \\(x_2\\) unchanged. Each \\(\\mathbf{x}\\) in the domain is a linear combination of the basis, and the image is also a linear combination of the images of the basis. Moreover, it wouldn’t make sense to interpret the problem as mapping \\(x_2\\) to \\(x_2-2x_1\\) since such transformation cannot change the dimensionality.",
    "crumbs": [
      "Part 1: Linear Equations in Linear Algebra",
      "Questions and Answers"
    ]
  },
  {
    "objectID": "part2/chap2.5.html",
    "href": "part2/chap2.5.html",
    "title": "2.5 Matrix Factorizations",
    "section": "",
    "text": "LU Factorization\nSince \\((E_m\\cdots E_1) A = U\\), \\(A = (E_m\\cdots E_1)^{-1} U\\). We can show that \\(L\\) is unique regardless that it’s a unit lower triangular matrix.\nNow, we need to prove that if \\(L\\) is a lower triangular matrix, operations to reduce \\(A\\) to \\(U\\) must not involve row interchange. If the operations include row interchanges, suppose \\(E_k\\) represents the last swap. After the \\(k\\)-th operation, there must exist an entry \\((E_k(E_{k-1}\\cdots E_1))_{pq}\\neq 0\\), where \\(p &lt; q\\). Because \\(E_k\\) is the last swap, \\((E_m\\cdots E_{k+1})\\) is a lower triangular matrix. It’s also possible to show that for every \\(1\\leq q' &lt; q\\), \\((E_k\\cdots E_1)_{pq'}=0\\). Thus, \\[(E_m\\cdots E_1)_{pq} = \\text{row}_p(E_m\\cdots E_{k+1})\\cdot \\text{col}_q(E_k\\cdots E_1)\\neq 0\\]\nCombined with the uniqueness of \\(L\\), \\(A\\) must be reducible to echelon form without row interchanges.",
    "crumbs": [
      "Part 2: Matrix Algebra",
      "2.5 Matrix Factorizations"
    ]
  },
  {
    "objectID": "part2/chap2.5.html#lu-factorization",
    "href": "part2/chap2.5.html#lu-factorization",
    "title": "2.5 Matrix Factorizations",
    "section": "",
    "text": "TipRemark\n\n\n\n\n\n\nRemark. Why \\(A\\) needs to be reducible to echelon form without row interchanges?\n\n\n\n\n\n\n\n\n\n\n\n\nProposition 1 False.\nIf \\(X\\) is invertible, there is a unique \\(X\\) that \\(XY=Z\\).\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof of Proposition 1 (False))\n\n\n\n\n\n\nProof (Proof of Proposition 1 (False)). If \\(X'Y\\) also equal \\(Z\\), \\[(X-X')Y=(Z-Z)=0\\] Left-multiply by \\(X^{-1}\\): \\[(I-X^{-1}X')Y=0\\]\nAssume \\(Y\\) and \\(Z\\) are non-zero. Then, \\[X^{-1}X'=I\\] \\[X=X'\\]\n\n\n\n\n\n\n\n\n\n\n\nProposition 2 If \\(X\\) is invertible, \\(X\\) is a lower triangular matrix if and only if \\(X^{-1}\\) is a lower triangular matrix.\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof of Proposition 2)\n\n\n\n\n\n\nProof (Proof of Proposition 2). If \\(X\\) is a lower triangular matrix, reducing \\(\\begin{bmatrix}X & I\\end{bmatrix}\\) to \\(\\begin{bmatrix}I & X^{-1}\\end{bmatrix}\\) makes \\(X^{-1}\\) inherit the lower triangular form.\nIf \\(X^{-1}\\) is a lower triangular matrix, \\((X^{-1})^{-1}=X\\) is a lower triangular matrix.",
    "crumbs": [
      "Part 2: Matrix Algebra",
      "2.5 Matrix Factorizations"
    ]
  },
  {
    "objectID": "part2/chap2.8.html",
    "href": "part2/chap2.8.html",
    "title": "2.8 Subspaces of R",
    "section": "",
    "text": "Column Space",
    "crumbs": [
      "Part 2: Matrix Algebra",
      "2.8 Subspaces of R"
    ]
  },
  {
    "objectID": "part2/chap2.8.html#column-space",
    "href": "part2/chap2.8.html#column-space",
    "title": "2.8 Subspaces of R",
    "section": "",
    "text": "Theorem 1 (Theorem 13) The pivot columns of a matrix \\(A\\) form a basis for the column space of \\(A\\).\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof of Theorem 1)\n\n\n\n\n\n\nProof (Proof of Theorem 1). Suppose \\(B\\) is the RREF of \\(A\\), then there’s a sequence of elementary matrices, \\(E_1,\\dots,E_p\\), where \\(E_p\\cdots E_1 A=B\\). Let \\(E=E_p\\cdots E_1\\), so we have \\(EA=B\\).\nDenote the columns of \\(A\\) and \\(B\\) as \\(\\mathbf{a}_k\\) and \\(\\mathbf{b}_k\\), respectively. The linear relationship in the columns of \\(A\\) is preserved in the columns of \\(B\\) because \\[\\begin{align}\nEA &= E\\begin{bmatrix}\\mathbf{a}_1 & \\cdots & \\mathbf{a}_n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}E\\mathbf{a}_1 & \\cdots & E\\mathbf{a}_n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\\mathbf{b}_1 & \\cdots & \\mathbf{b}_n\\end{bmatrix}\n\\end{align}\\] Thus, if \\(\\mathbf{a}_k = \\sum_{j\\neq k} c_j\\mathbf{a}_j\\), \\(\\mathbf{b}_k=E\\mathbf{a}_k=E\\sum_{j\\neq k} c_j \\mathbf{a}_j=\\sum_{j\\neq k} c_j E\\mathbf{a}_j=\\sum_{j\\neq k} c_j\\mathbf{b}_j\\).\nConversely, since \\(E\\) is invertible, we can prove the linear relationship in the columns of \\(B\\) is preserved in the columns of \\(A\\) by replacing \\(E\\) in the above proof with \\(E^{-1}\\) and swapping \\(A\\) and \\(B\\).\nTherefore, we have proved the linear relationship is the same in both \\(A\\) and \\(B\\).\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. It’s easy to show that every non-pivot column can be written as a linear combination of the prior pivot columns by the definition of RREF, so we can safely remove the non-pivot columns from the set of the columns without changing the span. Thus, we have completely proved Theorem 1.\nAlthough the pivot columns of \\(A\\) form a basis for \\(\\operatorname{Col} A\\), it’s not the only basis that can be formed from the columns. If we do row reduction in a different order of the columns, the pivot columns may be different.\n\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. It may be a bit confusing why we need to prove both directions here. We can formalize this proof using sets. Let \\(U\\) be the set that represents the linear relationship in \\(A\\), then \\(U=\\{(k,c_1,\\dots,c_n):\\mathbf{a}_k=c_1\\mathbf{a}_1+\\dots+c_{k-1}\\mathbf{a}_{k-1}+c_{k+1}\\mathbf{a}_{k+1}+\\dots+c_n\\mathbf{a}_n\\}\\). In the same way, we define \\(V\\) to be a set containing the linear relationship in \\(B\\).\nThe direction that is directly written in the above proof shows that for every combination \\(t\\in U\\), \\(t\\in V\\), so \\(U\\subseteq V\\). The opposite direction shows that \\(V\\subseteq U\\), so together we show that \\(U=V\\).",
    "crumbs": [
      "Part 2: Matrix Algebra",
      "2.8 Subspaces of R"
    ]
  },
  {
    "objectID": "part2/chap2.8.html#null-space",
    "href": "part2/chap2.8.html#null-space",
    "title": "2.8 Subspaces of R",
    "section": "Null Space",
    "text": "Null Space\n\n\n\n\n\n\nNoteDefinition (Null Space)\n\n\n\n\nDefinition 1 (Null Space) The null space of a matrix \\(A\\) is the set \\(\\operatorname{Nul} A\\) of all solutions of the homogeneous equation \\(A\\mathbf{x}=\\mathbf{0}\\).\n\n\n\n\n\n\n\n\n\n\nProposition 1 Row operations don’t change null space.\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof of Proposition 1)\n\n\n\n\n\n\nProof (Proof of Proposition 1). This is how we solve equations.\n\n\n\n\n\n\n\n\n\n\n\nProposition 2 Row operations don’t change row space, and the non-zero rows of the REF of a matrix form a basis of the row space.\n\n\n\n\n\n\n\n\n\n\n\nProposition 3 By some observation, \\(\\operatorname{Nul} A=\\bigcap_{i=1}^m \\operatorname{Nul}\\operatorname{row}_i(A)\\).\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof to Proposition 3 (Informal))\n\n\n\n\n\n\nProof (Proof to Proposition 3 (Informal)). The solution to the system of linear equations is also the solution to each linear equation.\n\n\n\n\n\n\n\n\n\n\n\nProposition 4 Each row is orthogonal to the null space of the row. The row space is orthogonal to the null space.\n\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\nProof. Since \\(A\\mathbf{x}=\\mathbf{0}\\), by the rule of matrix multiplication, \\(\\operatorname{row}_i(A)\\mathbf{x}=0\\) for all \\(1\\leq i\\leq m\\). Thus, each vector \\(x\\) in \\(\\operatorname{Nul} A\\) is orthogonal to the row space.\nThe first sentence is a special case of the second.\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. Let’s construct a basis for the null space of a row of \\(A\\).\nThe null space of the \\(i\\)-th row is \\(\\{(x_1,\\dots,x_n):r_1x_1+\\dots+r_nx_n=0\\}\\), where \\(r_j\\) is the \\(j\\)-th column of the \\(i\\)-th row of \\(A\\).\nSet \\(x_2=1\\) and \\(x_3,\\dots,x_n=0\\), we have \\((-\\frac{r_2}{r_1},1,0,\\dots,0)\\) as one vector in the basis.\nSimilarly, we can obtain the other \\(n-2\\) vectors in the basis.\n\\[(-\\frac{r_3}{r_1},0,1,0,0,\\dots,0)\\] \\[(-\\frac{r_4}{r_1},0,0,1,0,\\dots,0)\\] \\[\\cdots\\] \\[(-\\frac{r_n}{r_1},0,0,0,0,\\dots,1)\\]",
    "crumbs": [
      "Part 2: Matrix Algebra",
      "2.8 Subspaces of R"
    ]
  },
  {
    "objectID": "part2/chap2.8.html#linear-transformation",
    "href": "part2/chap2.8.html#linear-transformation",
    "title": "2.8 Subspaces of R",
    "section": "Linear Transformation",
    "text": "Linear Transformation\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. Consider a linear transformation \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\), where \\(T(\\mathbf{x})=A\\mathbf{x}\\). \\(\\operatorname{Col}A\\) is the range of the transformation, and \\(\\operatorname{Nul}A\\) is the set of \\(\\mathbf{x}\\in\\mathbb{R}^n\\) whose image under transformation \\(T\\) is \\(\\mathbf{0}\\). Thus, for every \\(T(\\mathbf{u})=\\mathbf{v}\\) and \\(\\mathbf{p}\\in\\operatorname{Nul}A\\), \\(T(\\mathbf{u}+\\mathbf{p})=\\mathbf{v}\\).",
    "crumbs": [
      "Part 2: Matrix Algebra",
      "2.8 Subspaces of R"
    ]
  },
  {
    "objectID": "part3/chap3.1.html",
    "href": "part3/chap3.1.html",
    "title": "3.1 Introduction to Determinants",
    "section": "",
    "text": "NoteDefinition (Determinant)\n\n\n\n\nDefinition 1 (Determinant) The textbook defines the determinant of a matrix to be the cofactor expansion of the first row. \\[ \\det A = a_{i1}C_{i1} + a_{i2}C_{i2} + \\dots + a_{in}C_{in} \\]\n\n\n\n\n\n\n\n\n\n\nTheorem 1 Informal. The cofactor expansion of any row or column equals the determinant of the matrix.\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. But how could all these cofactor expansions be the same? The textbook doesn’t give a proof, stating “We omit the proof of the following fundamental theorem to avoid a lengthy digression.” Since the book leans more on applications, it’s true that the proof is not much relevant, but the proof will give a more foundational understanding of determinants.\nTo prove Theorem 1, we show that there’s a unique determinant function. Then, we show that the cofactor expansion of any row or column equals the unique determinant function.\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof of Theorem 1)\n\n\n\n\n\n\nProof (Proof of Theorem 1). \n\n\n\n\n\n\n\nTheorem 2 (Uniqueness of the Determinant) There’s a unique function \\(D: \\mathbb{F}^{n\\times n} \\to \\mathbb{F}\\) that satisfies the following conditions, where we treat each row of the input matrix as a vector variable:\n\n\\(D\\) is multilinear in rows.\nIf two adjacent rows of \\(A\\) are equal, \\(D(A) = 0\\).\n\\(D(I) = 1\\).\n\n\n\n\n\n“Multilinear” means if there are two matrices that are only different in the \\(i\\)-th row, then\n\\[ D\\left(\\begin{bmatrix}\\vdots\\\\ \\mathbf{a}_i\\\\ \\vdots \\end{bmatrix}\\right) + D\\left(\\begin{bmatrix}\\vdots\\\\ \\mathbf{b}_i\\\\ \\vdots \\end{bmatrix}\\right) = D\\left(\\begin{bmatrix}\\vdots\\\\ \\mathbf{a}_i + \\mathbf{b}_i \\\\ \\vdots \\end{bmatrix}\\right) \\]\n\n\n\n\n\n\nNoteProof (Proof of Theorem 2)\n\n\n\n\n\n\nProof (Proof of Theorem 2). Some other textbooks prove the latter theorems such as \\(\\det AB = (\\det A)(\\det B)\\) and row operations first and use these theorems to prove the uniqueness. However, since this textbook relies on the cofactor expansion (which implies uniqueness) to prove those theorems, we can’t follow the same track.\nWe have to use the multilinearity to separate each element from the matrix. Let \\(A\\) be a \\(n\\times n\\) matrix, where \\(a_{ij}\\) denotes the element on the \\(i\\)-th row and \\(j\\)-th column. Let \\(\\bf{e}_j\\) denotes the row vector whose \\(j\\)-th entry is \\(1\\) and is the only non-zero entry.\n\\[\\begin{align}\nD(A) &= \\sum_{i}a_{1i} D\\L(\\mat{\\bf{e}_i \\\\ \\vdots}\\R) \\\\\n     &= \\sum_{i}a_{1i} \\sum_{j}a_{2j} D\\L(\\mat{\\bf{e}_i \\\\ \\bf{e}_j \\\\ \\vdots}\\R) \\\\\n     &= \\sum_{i}a_{1i} \\sum_{j}a_{2j} \\cdots \\sum_{p}a_{np} D\\L(\\mat{\\bf{e}_i \\\\ \\bf{e}_j \\\\ \\vdots \\\\ \\bf{e}_p}\\R) \\\\\n     &= \\sum_{i,j,\\dots,p} a_{1i}a_{2j}\\cdots a_{np} D\\L(\\mat{\\bf{e}_i \\\\ \\bf{e}_j \\\\ \\vdots \\\\ \\bf{e}_p}\\R)\n\\end{align}\\]\nFrom above derivation, we see each determinant can be decompose to a summation of the determinants of matrices whose rows are standard basis vectors. Then, we compute the value for those special matrices. The determinant of such matrices simply equals \\(D\\L(\\mat{\\bf{e}_i \\\\ \\bf{e}_j \\\\ \\vdots \\\\ \\bf{e}_p}\\R)\\) because other combinations include zero coefficients. Let’s denote this kind of matrices \\(E\\).\nSuppose we are interchanging two adjacent rows. We omit the other rows since they are equal. If \\(i \\neq j\\), then\n\\[\\begin{align}\nD\\L(\\mat{\\bf{e}_i+\\bf{e}_j \\\\ \\bf{e}_i+\\bf{e}_j}\\R) &= D\\L(\\mat{\\bf{e}_i \\\\ \\bf{e}_i}\\R) + D\\L(\\mat{\\bf{e}_i \\\\ \\bf{e}_j}\\R) + D\\L(\\mat{\\bf{e}_j \\\\ \\bf{e}_i}\\R) + D\\L(\\mat{\\bf{e}_j \\\\ \\bf{e}_j}\\R) \\\\\n&= D\\L(\\mat{\\bf{e}_i \\\\ \\bf{e}_j}\\R) + D\\L(\\mat{\\bf{e}_j \\\\ \\bf{e}_i}\\R) \\\\\n&= 0\n\\end{align}\\]\nOtherwise, the determinant is \\(0\\). Thus, interchanging adjacent rows of \\(E\\) negates the determinant. If \\(E\\) contains equal rows, by swapping rows, the determinant is \\(0\\). Otherwise, since \\(D(I)=1\\), \\(D(E) = (-1)^k\\), where \\(k\\) is the number of reverse pairs in the sequence of the positions of ones. This is because we need \\(k\\) times of adjacent interchanges to transform \\(I\\) to \\(E\\).\nNow we obtain the value of each basic component of \\(D\\). \\(\\qed\\)\n\n\n\n\nWe now prove cofactor expansions by a row satisfy multilinearity. Suppose we are applying cofactor expansion on the \\(k\\)-th row. If the \\(k\\)-th row is the row we operate, it’s trivial that multilinearity is satisfied. By mathematical induction, we prove the other case.\nBecause cofactor expansions are multilinear, to prove the second property, we again only need to verify that it is true for \\(E\\) (in the above proof). If two adjacent rows are equal, then all cofactors will be zero because they contain a zero row. Thus, the result of the cofactor expansion is also zero.\nThe third property is obviously true for cofactor expansions, so we prove that row cofactor expansions equal the determinant.\nWe now prove column cofactor expansions satisfy multilinearity. Either scalar multiplication or addition can be proven by dividing into two situations. First, for entries that are not on the selected row, this is true by induction. For the entry on the selected row, we split the coefficient.\nFor the second property, we again examine whether it’s true for \\(E\\). If the column contains more than a \\(1\\), then the cofactors will be zero by induction because there will be zero rows in the sub-matrices. Otherwise, we finish our proof by induction.\nAfter verifying the last property, we finally complete our proof for Theorem 1. \\(\\qed\\)",
    "crumbs": [
      "Part 3: Determinants",
      "3.1 Introduction to Determinants"
    ]
  },
  {
    "objectID": "part3/chap3.2x.html",
    "href": "part3/chap3.2x.html",
    "title": "3.2 [R] Number Theory?",
    "section": "",
    "text": "Quotient Spaces",
    "crumbs": [
      "Part 3: Determinants",
      "3.2 [R] Number Theory?"
    ]
  },
  {
    "objectID": "part3/chap3.2x.html#quotient-spaces",
    "href": "part3/chap3.2x.html#quotient-spaces",
    "title": "3.2 [R] Number Theory?",
    "section": "",
    "text": "Warning\n\n\n\nWaiting for the author to learn.",
    "crumbs": [
      "Part 3: Determinants",
      "3.2 [R] Number Theory?"
    ]
  },
  {
    "objectID": "part3/chap3.2x.html#bézouts-identity",
    "href": "part3/chap3.2x.html#bézouts-identity",
    "title": "3.2 [R] Number Theory?",
    "section": "Bézout’s Identity",
    "text": "Bézout’s Identity\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. Bézout’s identity states that for given \\(a\\) and \\(b\\), there exist \\(x\\) and \\(y\\) such that \\(ax+by = \\gcd(a,b)\\).\nConsider the determinant of \\(\\mat{m_1 & m_2 \\\\ x_1 & x_2}\\). \\[\\det\\mat{m_1 & m_2 \\\\ x_1 & x_2} = m_1x_2 - m_2x_1\\]\nIf the determinant is \\(1\\), which implies the bounded area by \\(\\mat{m_1 & m_2}\\) and \\(\\mat{x_1 & x_2}\\) is \\(1\\), then \\(m_1\\) and \\(m_2\\) are coprime. Additionally, \\[m_1x_2 \\equiv 1 \\pmod{m_2}\\] \\[-m_2x_1 \\equiv 1 \\pmod{m_1}\\]\nThus, \\(x_2\\) is the inverse of \\(m_1\\) modulo \\(m_2\\), and \\(-x_1\\) is the inverse of \\(m_2\\) modulo \\(m_1\\).",
    "crumbs": [
      "Part 3: Determinants",
      "3.2 [R] Number Theory?"
    ]
  },
  {
    "objectID": "part4/chap4.6.html",
    "href": "part4/chap4.6.html",
    "title": "4.6 Rank",
    "section": "",
    "text": "For this chapter, all vector spaces are finite unless stated explicitly.\nThis chapter in the textbook introduces the concept of row spaces and characteristics of it. Early in this note, Chapter 2.8 Subspaces explores some of its interesting properties, one of them being the following proposition.\n\n\n\n\n\n\n\nProposition 1 Row operations don’t change the row space, and the non-zero rows of the REF of a matrix form a basis of the row space.\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof of Proposition 1)\n\n\n\n\n\n\nProof (Proof of Proposition 1). Suppose \\(E\\) is a elementary matrix, and \\(A\\sim EA\\).\nThen each row of \\(EA\\) is a linear combination of the rows in \\(A\\). Thus, each vector in \\(\\opn{Row}(EA)\\) is automatically in \\(\\opn{Row}A\\).\nConversely, \\(EA \\sim E^{-1}(EA)\\), and the same holds. Each vector in \\(\\opn{Row}(E^{-1}(EA))=\\opn{Row}A\\) is in \\(\\opn{Row}(EA)\\), so \\(\\opn{Row}A = \\opn{Row}(EA)\\).\n\n\n\n\nThe textbook provides an example to visualize the relation between \\(\\opn{Nul} A\\) and \\(\\opn{Row} A\\) and between \\(\\opn{Nul} A^T\\) and \\(\\opn{Col} A\\).\nIn the below two examples, \\(A = \\displaystyle\\mat{3 & 0 & -1 \\\\ 3 & 0 & -1 \\\\ 4 & 0 & 5}\\).\n\n\n\n\n\n\nTip\\(\\opn{Nul} A\\) and \\(\\opn{Row} A\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\\(\\opn{Nul} A^T\\) and \\(\\opn{Col} A\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. We observe that two spaces in each example are perpendicular to each other. In fact, this can be generalized to all matrices of higher dimensions.\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\nProof. For all \\(\\bf{v} \\in \\opn{Nul} A\\), \\(\\displaystyle{A\\bf{v}=\\mat{\\opn{row}_1(A) \\\\ \\opn{row}_2(A) \\\\ \\vdots \\\\ \\opn{row}_m(A)}\\bf{v}=\\bf{0}}\\). Thus, for all \\(1\\leq j\\leq m\\), \\(\\opn{row}_j(A) \\cdot \\bf{v}=0\\), so the two spaces are othrogonal to each other.\n\n\n\n\nTherefore, \\(\\opn{Col}A = \\opn{Row}A^T \\perp \\opn{Nul}A^T\\) as shown in the visualizations.\nThe orthogonal relation leads to the exploration of the following proposition.\n\n\n\n\n\n\n\n\n\n\n\nProposition 2 Given vector spaces \\(U\\) and \\(V\\), where \\(V \\subseteq U \\subseteq \\bb{R}^n\\). There’s a unique subspace of \\(U\\), \\(W\\), which satisfies that \\(V+W = U\\) and \\(V \\perp W\\).\n\n\n\n\n\n\n\n\n\n\nNoteProof (Proof of Proposition 2)\n\n\n\n\n\n\nProof (Proof of Proposition 2). Based on knowledge available so far, we only prove the proposition is true when \\(U\\) is a subspace of \\(\\bb{R}^n\\), but it’s true for all finite inner product spaces.\nA natural idea is to use the fact that \\(\\opn{Nul}A \\perp \\opn{Row}A\\).\nOur first attempt is to construct a matrix \\(A\\) with rows being a basis of \\(V\\). Therefore, \\(\\opn{Row}A=V\\) and \\(V \\perp \\opn{Nul}A\\). If we could separate a subspace \\(W\\) satisfying the conditions, we would prove it. However, \\(U\\) is a subspace of \\(\\bb{R}^n\\), which means we only know that \\(U \\subseteq V+\\opn{Nul}A\\). The normal way to obtain a basis of \\(\\opn{Nul}A\\) by solving the system does not guarantee that, by choosing vectors in the basis that are in \\(U\\), the spanned space \\(W\\) would satisfy that \\(V+W=U\\).\nBelow is an example with \\(U=\\opn{span}\\{(1, 0, 0), (0, 1, 1)\\}, V=\\opn{span}\\{(1, 0, 0)\\}\\) and the basis of \\(\\opn{Nul}A\\) being \\(\\bf{v}_1=(0, -1, 0),\\bf{v}_2=(0, 0, 1)\\).\n\n\n\n\n\n\nTipA Counterexample\n\n\n\n\n\n\n\n\n\n\n\nIn this case, neither \\(W=\\opn{span}\\{\\bf{v}_1\\}\\) nor \\(W=\\opn{span}\\{\\bf{v}_2\\}\\) would be a valid choice.\nEventually, coordinate mappings lead to a successful proof.\n\n\n\n\n\n\n\nTheorem 1 Let \\(\\mathcal{B}=\\left\\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\right\\}\\) be a basis for a vector space \\(V\\). Then the coordinate mapping \\(\\mathbf{x} \\mapsto[\\mathbf{x}]_{\\mathcal{B}}\\) is a one-to-one linear transformation from \\(V\\) onto \\(\\mathbb{R}^n\\).\n\n\n\n\nThe theorem is proved in the textbook, and we would not reiterate the proof here. If we can find a coordinate mapping \\(\\bf{x} \\mapsto [\\bf{x}]_\\cal{B}\\) which preserves the orthogonality, by proving the proposition for the coordinate vectors, we complete our proof by mapping \\(U\\) onto \\(\\bb{R}^{\\dim U}\\). Let \\(\\dim U = m\\).\nWe find the orthogonal basis by starting with an empty set. Each time, we add a vector in \\(U\\) that is orthogonal to all selected vectors in the basis. Let \\(A_i = \\mat{\\bf{b}_1^T \\\\ \\vdots \\\\ \\bf{b}_{i}^T}\\). If at any step \\(i &lt; m\\), we cannot find such vector to add, then for all \\(\\bf{u} \\in U\\), \\(\\bf{u} \\notin \\opn{Nul}A_i\\). In addition, we have \\(\\opn{Row}A_i \\subsetneqq U\\), so \\(\\dim\\opn{Row}A_i &lt; \\dim U\\). \\[\\begin{align}\n\\dim(U+\\opn{Nul}A_i) &= \\dim U + \\dim\\opn{Nul}A_i \\\\\n&&gt; \\dim\\opn{Row}A_i + \\dim\\opn{Nul}A_i \\\\\n&= n\n\\end{align}\\] But \\(\\dim(U+\\opn{Nul}A_i) \\leq n\\) because \\(U+\\opn{Nul}A_i\\subseteq \\bb{R}^n\\). Therefore, it’s always possible to construct an orthogonal basis for \\(U\\). Lastly, we normalize each vector so that they all have a length of \\(1\\). Denote the new basis as \\(\\cal{B}'=\\{\\bf{b}'_1,\\dots,\\bf{b}'_m\\}\\).\nSuppose \\(\\bf{u}, \\bf{v} \\in U\\). Let \\([\\bf{u}]_{\\cal{B}'}=\\mat{u'_1 \\\\ \\vdots \\\\ u'_m}, [\\bf{v}]_{\\cal{B}'}=\\mat{v'_1 \\\\ \\vdots \\\\ v'_m}\\). We show that the dot product of \\(\\bf{u}\\) and \\(\\bf{v}\\) is zero iff the dot product of \\([\\bf{u}]_{\\cal{B}'}\\) and \\([\\bf{v}]_{\\cal{B}'}\\) is zero.\n\\[\\begin{align}\n\\bf{u}^T\\bf{v} &= (u'_1\\bf{b}'_1 + \\dots + u'_m\\bf{b}'_m)^T(v'_1\\bf{b}'_1 + \\dots + v'_m\\bf{b}'_m) \\\\\n&= (u'_1{\\bf{b}'_1}^T + \\dots + u'_m{\\bf{b}'_m}^T)(v'_1\\bf{b}'_1 + \\dots + v'_m\\bf{b}'_m) \\\\\n&= \\sum_{i,j} u'_i v'_j {\\bf{b}'_i}^T \\bf{b}'_j \\\\\n&= \\sum_{i} u'_i v'_i {\\bf{b}'_i}^T \\bf{b}'_i \\\\\n&= \\sum_{i} u'_i v'_i \\\\\n&= [\\bf{u}]_{\\cal{B}'}^T [\\bf{v}]_{\\cal{B}'}\n\\end{align}\\]\nLines 3 and 4 are equal because \\(\\bf{b}'_i\\) and \\(\\bf{b}'_j\\) are orthogonal when \\(i \\neq j\\). Line 4 and Line 5 are equal because we have normalized the vectors in \\(\\cal{B}'\\); thus, \\({\\bf{b}'_i}^T\\bf{b}'_i=\\|\\bf{b}'_i\\|^2=1\\).\nLet \\(M=\\mat{[\\bf{t}_1]_{\\cal{B}'} \\\\ \\vdots \\\\ [\\bf{t}_{\\dim V}]_{\\cal{B}'}}\\), where \\(\\{\\bf{t}_1,\\dots,\\bf{t}_{\\dim V}\\}\\) is an arbitrary basis of \\(V\\). Applying the transformation \\([\\bf{x}]_{\\cal{B}'} \\mapsto \\bf{x}\\) on \\(\\opn{Nul}M\\) gives the subspace \\(W\\). A subspace including exclusively all vectors that are orthogonal to \\(\\opn{Row}M\\) is automatically \\(\\opn{Nul}M\\) because the dot product of each row of \\(M\\) and \\(\\bf{v}\\) being zero implies that \\(M\\bf{v}=\\bf{0}\\), concluding the proof.\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. It is possible to construct a unit orthogonal basis for any vector space \\(U \\subseteq \\bb{R}^n\\).\n\n\n\n\n\n\n\n\n\n\nNoteProof (An Alternative Proof of Proposition 2)\n\n\n\n\n\n\nProof (An Alternative Proof of Proposition 2). Starting with a matrix with its rows being an arbitrary basis for \\(V\\), \\(\\mat{\\bf{v}_1^T \\\\ \\vdots \\\\ \\bf{v}_m^T}\\), we show that there exists a basis for \\(W\\), \\(\\bf{w}_1,\\dots,\\bf{w}_p\\). Similar to above proof, we define \\(A_i = \\mat{\\bf{v}_1^T \\\\ \\vdots \\\\ \\bf{w}_i^T}\\). For step \\(i\\), we find a \\(\\bf{w}_{i+1} \\neq \\bf{0} \\in U\\) such that \\(\\opn{row}_k(A_i) \\bf{w}_{i+1} = 0\\) for all rows in \\(A_i\\). That is, \\(A_i \\bf{w}_{i+1} = \\bf{0}\\).\nIf at any step \\(i &lt; p\\), where \\(p = \\dim U - m\\), there does not exist such \\(\\bf{w}_{i+1}\\), then \\(U \\cap \\opn{Nul}A_i = \\{\\bf{0}\\}\\), so \\(\\dim(U + \\opn{Nul}A_i) = \\dim U + \\dim\\opn{Nul}A_i\\). \\[\\begin{align}\n\\dim(U+\\opn{Nul}A_i) &= \\dim U + \\dim\\opn{Nul}A_i \\\\\n&&gt; \\dim\\opn{Row}A_i + \\dim\\opn{Nul}A_i \\\\\n&= n\n\\end{align}\\] But \\(\\dim(U+\\opn{Nul}A_i) \\leq n\\) because \\(U+\\opn{Nul}A_i\\subseteq \\bb{R}^n\\). Hence, there exists \\(W=\\opn{span}\\{\\bf{w}_1,\\dots,\\bf{w}_p\\}\\). It’s easy to check \\(W\\) satisfies the requirements.\nWe now prove that any two subspaces \\(W, W'\\) satisfying the stated conditions are equal. Note that \\(W\\) here is not necessarily the above \\(W\\) obtained by construction.\nLet \\(\\bf{w}'_1,\\dots,\\bf{w}'_p\\) be a basis for \\(W'\\). Because \\(V+W'=U\\), for all \\(\\bf{w} \\in W\\), \\(\\bf{w} \\in U\\), so \\[\\bf{w} = c_1\\bf{v}_1+\\dots+c_m\\bf{v}_m + d_1\\bf{w}'_1+\\dots+d_p\\bf{w}'_p\\]\nLet \\(M=\\mat{\\bf{v}_1^T\\\\\\vdots\\\\\\bf{v}_m^T}\\). Since \\(V\\perp W\\) and \\(V\\perp W'\\), \\[\\begin{align}\nM\\bf{w} &= M(c_1\\bf{v}_1+\\dots+c_m\\bf{v}_m + d_1\\bf{w}'_1+\\dots+d_p\\bf{w}'_p) \\\\\n&= M(c_1\\bf{v}_1+\\dots+c_m\\bf{v}_m) \\\\\n&= \\bf{0}\n\\end{align}\\] Since \\(c_1\\bf{v}_1+\\dots+c_m\\bf{v}_m\\) is in both \\(\\opn{Row}M\\) and \\(\\opn{Nul}M\\), and a basis is linearly independent, \\(\\bf{c}=\\bf{0}\\). Thus, \\(\\bf{w}=d_1\\bf{w}'_1+\\dots+d_p\\bf{w}'_p \\in W'\\). It follows that \\(W\\subseteq W'\\).\nConversely, \\(W' \\subseteq W\\) is automatically true since \\(W\\) and \\(W'\\) are arbitrary subspaces fulfulling the requirements. Therefore, there is a unique \\(W\\) such that \\(V+W=U\\) and \\(V\\perp W\\). \\(\\qed\\)",
    "crumbs": [
      "Part 4: Vector Spaces",
      "4.6 Rank"
    ]
  },
  {
    "objectID": "part4/chap4.x.html",
    "href": "part4/chap4.x.html",
    "title": "Miscellaneous",
    "section": "",
    "text": "4.4 Coordinate Systems",
    "crumbs": [
      "Part 4: Vector Spaces",
      "Miscellaneous"
    ]
  },
  {
    "objectID": "part4/chap4.x.html#coordinate-systems",
    "href": "part4/chap4.x.html#coordinate-systems",
    "title": "Miscellaneous",
    "section": "",
    "text": "Theorem 1 (Theorem 8) Let \\(\\mathcal{B}=\\left\\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\right\\}\\) be a basis for a vector space \\(V\\). Then the coordinate mapping \\(\\mathbf{x} \\mapsto[\\mathbf{x}]_{\\mathcal{B}}\\) is a one-to-one linear transformation from \\(V\\) onto \\(\\mathbb{R}^n\\).\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. What this theorem means is that for a transformation \\(T: V \\to \\mathbb{R}^n\\) which is defined as \\(\\mathbf{x} \\mapsto [\\mathbf{x}]_{\\mathcal{B}}\\), \\(T\\) is a linear transformation and is both one-to-one and onto.\n\n\n\n\n\n\n\n\n\n\n\nExercise 1 (Practice Problem 2) The set \\(\\mathcal{B}=\\left\\{1+t, 1+t^2, t+t^2\\right\\}\\) is a basis for \\(\\mathbb{P}_2\\). Find the coordinate vector of \\(\\mathbf{p}(t)=6+3 t-t^2\\) relative to \\(\\mathcal{B}\\).\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. One way to solve this is to equate the coefficients of the equation \\[c_1(1+t) + c_2(1+t^2) + c_3(t+t^2)=6+3 t-t^2\\] and solve for \\(c_1,c_2,c_3\\).\nHowever, by observation, we can also first write \\(1+t,1+t^2,t+t^2,6+3t-t^2\\) as \\(\\begin{bmatrix}1\\\\ 1\\\\ 0\\end{bmatrix}, \\begin{bmatrix}1\\\\ 0\\\\ 1\\end{bmatrix}, \\begin{bmatrix}0\\\\ 1\\\\ 1\\end{bmatrix}, \\begin{bmatrix}6\\\\ 3\\\\ -1\\end{bmatrix}\\). Then, as for subspaces in \\(\\mathbb{R}\\), we solve the coefficients \\(\\mathbf{c}\\) using row reduction.\n\n\n\n\n\n\n\n\n\n\nTipWhy does this method work?\n\n\n\nLet \\(T: \\mathbf{x} \\mapsto [\\mathbf{x}]_\\mathcal{B}\\), so \\(T\\) is a bijective linear transformation.\n\\[\\begin{align}\nT(c_1\\mathbf{b}_1 + \\cdots + c_n\\mathbf{b}_n) &= c_1 T(\\mathbf{b}_1) + \\cdots + c_n T(\\mathbf{b}_n) \\\\\n&= T(\\mathbf{p})\n\\end{align}\\]\nwhere \\(T(\\mathbf{b}_1),\\dots,T(\\mathbf{b}_n)\\) are the coordinate vectors of the basis. Since \\(T\\) is bijective, we can apply \\(T^{-1}\\) to both side of the equation. Reversing the steps gives the same coefficients.",
    "crumbs": [
      "Part 4: Vector Spaces",
      "Miscellaneous"
    ]
  },
  {
    "objectID": "part4/chap4.x.html#isomorphism",
    "href": "part4/chap4.x.html#isomorphism",
    "title": "Miscellaneous",
    "section": "Isomorphism",
    "text": "Isomorphism\n\n\n\n\n\n\nTipHow to write rigorous solutions when using coordinate mappings and isomorphism?\n\n\n\nIn some problems, we are given a basis of a vector space other than a subspace of \\(\\bb{R}^n\\). One of the most frequent ones is about polynomials, such as Exercise 1.\nA formalized version of this type of problems: Given basis \\(\\cal{B}=\\{\\bf{b}_1, \\dots, \\bf{b}_n\\}\\) and \\(\\bf{v}\\) in vector space \\(V\\), find \\([\\bf{v}]_\\cal{B}\\).\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\nProof. Define transformation \\(T: \\bf{x} \\mapsto [\\bf{x}]_\\cal{E}\\), where \\(\\cal{E}\\) is another basis of vector space \\(V\\) (usually \\(\\{1,x,x^2,\\dots\\}\\) for polynomial spaces).\nSolve \\(\\mat{[\\bf{b}_1]_\\cal{E} & \\cdots & [\\bf{b}_n]_\\cal{E}}\\bf{x} = [\\bf{v}]_\\cal{E}\\) for \\(\\bf{x}\\). Then,\n\\[x_1[\\bf{b}_1]_\\cal{E} + \\dots + x_n[\\bf{b}_n]_\\cal{E} = [\\bf{v}]_\\cal{E}\\]\nBecause \\(T\\) is a linear transformation, \\([x_1\\bf{b}_1+\\dots+x_n\\bf{b}_n]_\\cal{E}=[\\bf{v}]_\\cal{E}\\).\nSince \\(T\\) is one-to-one, \\([\\bf{u}]_\\cal{E}=[\\bf{v}]_\\cal{E}\\) if and only if \\(\\bf{u}=\\bf{v}\\). Therefore, \\(\\bf{v}=x_1\\bf{b}_1+\\dots+x_n\\bf{b}_n\\), \\[[\\bf{v}]_\\cal{B}=\\bf{x}\\]",
    "crumbs": [
      "Part 4: Vector Spaces",
      "Miscellaneous"
    ]
  },
  {
    "objectID": "part4/chap4.x.html#solving-for-bases-using-column-operations",
    "href": "part4/chap4.x.html#solving-for-bases-using-column-operations",
    "title": "Miscellaneous",
    "section": "Solving for Bases Using Column Operations",
    "text": "Solving for Bases Using Column Operations\nThe standard method to obtain a basis for either column spaces, row spaces, or null spaces is performing row reductions.\nLet \\(B\\) be the RREF of \\(A\\). The columns in \\(A\\) that correspond with a pivot column in \\(B\\) form a basis for \\(\\opn{Col}A\\), the non-zero rows of \\(B\\) form a basis for \\(\\opn{Row}A\\), and the parametric vector form of its solution set we derive from \\(B\\) indicates a basis for \\(\\opn{Nul}A\\).\n\nMy friend: Why should I perform row reductions? Why not column operations?\nMe: If you clearly know what you are doing, you can solve them using column operations.\n— during a linear algebra class.\n\nUsing column operations to solve for the column space and the row space is simply doing row reduction on the transpose of the original matrix. Therefore, the non-zero columns of the reduced column echelon form (RCEF) give a basis for the column space, and the pivot rows in the original matrix constitute a basis for the row space.\nHowever, null spaces are different because they are defined as \\(\\{\\bf{x} : A\\bf{x}=\\bf{0}\\}\\) rather than explicitly the span of some vectors. The idea of transpose doesn’t work here because its counterpart is \\(\\opn{Nul}A^T\\), which even complicates the problem.\nSimilar to row operations, column operations can also be expressed by right-multiplying elementary matrices. Let the RCEF of a matrix \\(A\\) with \\(n\\) columns, \\(B=A E_1 \\cdots E_p = AM\\), where \\(E_1,\\dots,E_p\\) are the elementary matrices corresponding to the column operations. Observe that \\(B\\) has the following form: \\[\\mat{1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ \\ast & \\ast & 0 & 0 \\\\ 0 & 0 & 1 & 0}\\]\nLet \\(r=\\opn{rank}A\\), then \\(x_1=\\cdots=x_r=0\\) for equation \\(B\\bf{x}=(AM)\\bf{x}=\\bf{0}\\), while \\(x_{r+1},\\dots,x_n\\) are free variables completely independent of \\(x_1,\\dots,x_r\\). Therefore, we can write a basis for \\(\\opn{Nul}(AM)\\)—\\(\\{\\bf{e}_{r+1},\\dots,\\bf{e}_n\\}\\).\nSince elementary matrices are invertible, \\(M\\) is also invertible. We show that \\(\\{M\\bf{e}_{r+1},\\dots,M\\bf{e}_n\\}\\) is a basis for \\(\\opn{Nul}A\\).\n\nSpan: For all \\(\\bf{v} \\in \\opn{Nul}A\\), \\(A\\bf{v}=(AM)(M^{-1}\\bf{v})=\\bf{0}\\), so \\(M^{-1}\\bf{v}\\in\\opn{Nul}(AM)\\)\n\n\\[\\begin{align}\n\\bf{v}&=M(M^{-1}\\bf{v}) \\\\\n&=M(c_{r+1}\\bf{e}_{r+1}+\\dots+c_n\\bf{e}_n) \\\\\n&=c_{r+1}(M\\bf{e}_{r+1})+\\dots+c_n(M\\bf{e}_n)\n\\end{align}\\]\n\nLinear Independence: By contradiction, suppose it is linearly dependent. There exists \\(\\bf{x}\\neq\\bf{0}\\) such that \\[\\mat{M\\bf{e}_{r+1} & \\cdots & M\\bf{e}_n}\\bf{x}=\\bf{0}\\] Left-multiply the equation by \\(M^{-1}\\), \\[\\begin{align}\nM^{-1}\\mat{M\\bf{e}_{r+1} & \\cdots & M\\bf{e}_n}\\bf{x}&=\\mat{M^{-1}M\\bf{e}_{r+1} & \\cdots & M^{-1}M\\bf{e}_n}\\bf{x} \\\\\n&=\\mat{\\bf{e}_{r+1} & \\cdots & \\bf{e}_n}\\bf{x} \\\\\n&=M^{-1}\\bf{0}=\\bf{0}\n\\end{align}\\] Since \\(\\bf{x}\\neq\\bf{0}\\), this leads to that \\(\\{\\bf{e}_{r+1},\\dots,\\bf{e}_n\\}\\), a basis for \\(\\opn{Nul}(AM)\\), is linear dependent.\n\nTherefore, \\(\\{M\\bf{e}_{r+1},\\dots,M\\bf{e}_n\\}\\) forms a basis for \\(\\opn{Nul}A\\). Specifically, these vectors are the \\(r+1\\)-th column to the \\(n\\)-th column of matrix \\(M\\).\nOne problem remains: how to obtain \\(M\\)? Using the same logic of finding the inverse of a matrix, we augment the original matrix with an identity matrix below \\(A\\). \\[\\mat{A \\\\ I}\\sim\\mat{B \\\\ M}\\] As \\(A\\) is reduced to the reduced column echelon form, the identity matrix is transformed to \\(M\\).\n\nAn Example\n\\[\\mat{1 & 2 & 3 \\\\ 2 & 4 & 6 \\\\ 1 & 1 & 1}\\]\n\n\n\n\n\n\nNoteSolution (Row Operations)\n\n\n\n\n\n\nSolution (Row Operations). \\[\\mat{1 & 2 & 3 \\\\ 2 & 4 & 6 \\\\ 1 & 1 & 1}\\sim \\mat{1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0}\\]\nTherefore, \\(\\left\\{\\mat{1 \\\\ -2 \\\\ 1}\\right\\}\\) is a basis for \\(\\opn{Nul}A\\).\n\n\n\n\n\n\n\n\n\n\nNoteSolution (Column Operations)\n\n\n\n\n\n\nSolution (Column Operations). \\[\\mat{1 & 2 & 3 \\\\ 2 & 4 & 6 \\\\ 1 & 1 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1}\\sim\\mat{1 & 0 & 0 \\\\ 2 & 0 & 0 \\\\ 1 & -1 & -2 \\\\ 1 & -2 & -3 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1}\\sim\\mat{1 & 0 & 0 \\\\ 2 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & -1 & -2 \\\\ 0 & 0 & 1}\\]\nThe third column \\(\\mat{1 \\\\ -2 \\\\ 1}\\) gives the exactly same result as what we obtain using row operations.\n\n\n\n\n\n\n\n\n\n\nTipRemark\n\n\n\n\n\n\nRemark. In fact, we may only reduce \\(A\\) to the column echelon form since all operations required to transform a matrix from its column echelon form to its reduced column echelon form do not change the \\(r+1\\)-th to the \\(n\\)-th columns of \\(M\\).",
    "crumbs": [
      "Part 4: Vector Spaces",
      "Miscellaneous"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Lay, David C, Steven R Lay, and Judith McDonald. 2016. Linear\nAlgebra and Its Applications. 5th ed. Pearson.\n\n\n\\[\\begin{align}\\end{align}\\]",
    "crumbs": [
      "References"
    ]
  }
]