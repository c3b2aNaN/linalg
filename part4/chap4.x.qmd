# Miscellaneous {.unnumbered}

## 4.4 Coordinate Systems

::: {#thm-4-8}

## Theorem 8

Let $\mathcal{B}=\left\{\mathbf{b}_1, \ldots, \mathbf{b}_n\right\}$ be a basis for a vector space $V$. Then the coordinate mapping $\mathbf{x} \mapsto[\mathbf{x}]_{\mathcal{B}}$ is a one-to-one linear transformation from $V$ onto $\mathbb{R}^n$.
:::

::: {.remark}

What this theorem means is that for a transformation $T: V \to \mathbb{R}^n$ which is defined as $\mathbf{x} \mapsto [\mathbf{x}]_{\mathcal{B}}$, $T$ is a linear transformation and is both one-to-one and onto.
:::

::: {#exr-4-4-p2}

## Practice Problem 2

The set $\mathcal{B}=\left\{1+t, 1+t^2, t+t^2\right\}$ is a basis for $\mathbb{P}_2$. Find the coordinate vector of $\mathbf{p}(t)=6+3 t-t^2$ relative to $\mathcal{B}$.
:::

::: {.remark}

One way to solve this is to equate the coefficients of the equation 
$$c_1(1+t) + c_2(1+t^2) + c_3(t+t^2)=6+3 t-t^2$$
and solve for $c_1,c_2,c_3$.

However, by observation, we can also first write $1+t,1+t^2,t+t^2,6+3t-t^2$ as $\begin{bmatrix}1\\ 1\\ 0\end{bmatrix}, \begin{bmatrix}1\\ 0\\ 1\end{bmatrix}, \begin{bmatrix}0\\ 1\\ 1\end{bmatrix}, \begin{bmatrix}6\\ 3\\ -1\end{bmatrix}$. Then, as for subspaces in $\mathbb{R}$, we solve the coefficients $\mathbf{c}$ using row reduction.
:::

::: {.callout-tip}

## Why does this method work?

Let $T: \mathbf{x} \mapsto [\mathbf{x}]_\mathcal{B}$, so $T$ is a bijective linear transformation.

\begin{align}
T(c_1\mathbf{b}_1 + \cdots + c_n\mathbf{b}_n) &= c_1 T(\mathbf{b}_1) + \cdots + c_n T(\mathbf{b}_n) \\
&= T(\mathbf{p})
\end{align}

where $T(\mathbf{b}_1),\dots,T(\mathbf{b}_n)$ are the coordinate vectors of the basis. Since $T$ is bijective, we can apply $T^{-1}$ to both side of the equation. Reversing the steps gives the same coefficients.

<!-- First, we formalize our original problem as follows: we aim to find the value of $T(\mathbf{p})$, where $T: \mathbf{p} \mapsto [\mathbf{p}]_\mathcal{B}$. If we knows $T_1: \mathbf{p} \mapsto [\mathbf{p}]_{\mathcal{B}'}$ and $T_2: [\mathbf{p}]_{\mathcal{B}'} \mapsto [\mathbf{p}]_{\mathcal{B}}$, then $T(\mathbf{p})=T_2(T_1(\mathbf{p}))$.

In our case, $\mathcal{B}'=\{1,t,t^2\}$, so $T_1$ is obvious. For each $\mathbf{b}_k \in \mathcal{B}$,

\begin{align}
T(\mathbf{b}_k)&=\mathbf{e}_k \\
&=T_2(T_2^{-1}(\mathbf{e}_k)) \\
&=T_2(T_2^{-1}([\mathbf{b}_k]_\mathcal{B})) \\
&=T_2([\mathbf{b}_k]_{\mathcal{B}'}) \\
&=T_2(T_1(\mathbf{b}_k))
\end{align}

Since $T,T_1,T_2$ are linear transformations, by proving that they yield the same result for each vector in their basis, we have shown that $T$ is equivalent to applying $T_1$ and then $T_2$. -->
:::

## Isomorphism

::: {.callout-tip}

## How to write rigorous solutions when using coordinate mappings and isomorphism?

In some problems, we are given a basis of a vector space other than a subspace of $\bb{R}^n$. One of the most frequent ones is about polynomials, such as [@exr-4-4-p2].

A formalized version of this type of problems: Given basis $\cal{B}=\{\bf{b}_1, \dots, \bf{b}_n\}$ and $\bf{v}$ in vector space $V$, find $[\bf{v}]_\cal{B}$.

::: {.proof}

Define transformation $T: \bf{x} \mapsto [\bf{x}]_\cal{E}$, where $\cal{E}$ is another basis of vector space $V$ (usually $\{1,x,x^2,\dots\}$ for polynomial spaces).

Solve $\mat{[\bf{b}_1]_\cal{E} & \cdots & [\bf{b}_n]_\cal{E}}\bf{x} = [\bf{v}]_\cal{E}$ for $\bf{x}$. Then,

$$x_1[\bf{b}_1]_\cal{E} + \dots + x_n[\bf{b}_n]_\cal{E} = [\bf{v}]_\cal{E}$$

**Because $T$ is a linear transformation**, $[x_1\bf{b}_1+\dots+x_n\bf{b}_n]_\cal{E}=[\bf{v}]_\cal{E}$.

**Since $T$ is one-to-one**, $[\bf{u}]_\cal{E}=[\bf{v}]_\cal{E}$ if and only if $\bf{u}=\bf{v}$. Therefore, $\bf{v}=x_1\bf{b}_1+\dots+x_n\bf{b}_n$,
$$[\bf{v}]_\cal{B}=\bf{x}$$
:::
:::

## Solving for Bases Using Column Operations

The standard method to obtain a basis for either column spaces, row spaces, or null spaces is performing row reductions.

Let $B$ be the RREF of $A$. The columns in $A$ that correspond with a pivot column in $B$ form a basis for $\opn{Col}A$, the non-zero rows of $B$ form a basis for $\opn{Row}A$, and the parametric vector form of its solution set we derive from $B$ indicates a basis for $\opn{Nul}A$.

> *My friend: Why should I perform row reductions? Why not column operations?* \
> *Me: If you clearly know what you are doing, you can solve them using column operations.* \
> *--- during a linear algebra class.*

Using column operations to solve for the column space and the row space is simply doing row reduction on the transpose of the original matrix. Therefore, the non-zero columns of the reduced column echelon form (RCEF) give a basis for the column space, and the pivot rows in the original matrix constitute a basis for the row space.

However, null spaces are different because they are defined as $\{\bf{x} : A\bf{x}=\bf{0}\}$ rather than explicitly the span of some vectors. The idea of transpose doesn't work here because its counterpart is $\opn{Nul}A^T$, which even complicates the problem.

Similar to row operations, column operations can also be expressed by right-multiplying elementary matrices. Let the RCEF of a matrix $A$ with $n$ columns, $B=A E_1 \cdots E_p = AM$, where $E_1,\dots,E_p$ are the elementary matrices corresponding to the column operations. Observe that $B$ has the following form:
$$\mat{1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ \ast & \ast & 0 & 0 \\ 0 & 0 & 1 & 0}$$

Let $r=\opn{rank}A$, then $x_1=\cdots=x_r=0$ for equation $B\bf{x}=(AM)\bf{x}=\bf{0}$, while $x_{r+1},\dots,x_n$ are free variables completely independent of $x_1,\dots,x_r$. Therefore, we can write a basis for $\opn{Nul}(AM)$---$\{\bf{e}_{r+1},\dots,\bf{e}_n\}$. 

Since elementary matrices are invertible, $M$ is also invertible. We show that $\{M\bf{e}_{r+1},\dots,M\bf{e}_n\}$ is a basis for $\opn{Nul}A$.

- Span: For all $\bf{v} \in \opn{Nul}A$, $A\bf{v}=(AM)(M^{-1}\bf{v})=\bf{0}$, so $M^{-1}\bf{v}\in\opn{Nul}(AM)$

\begin{align}
\bf{v}&=M(M^{-1}\bf{v}) \\
&=M(c_{r+1}\bf{e}_{r+1}+\dots+c_n\bf{e}_n) \\
&=c_{r+1}(M\bf{e}_{r+1})+\dots+c_n(M\bf{e}_n)
\end{align}

- Linear Independence: By contradiction, suppose it is linearly dependent. There exists $\bf{x}\neq\bf{0}$ such that
$$\mat{M\bf{e}_{r+1} & \cdots & M\bf{e}_n}\bf{x}=\bf{0}$$
Left-multiply the equation by $M^{-1}$, 
\begin{align}
M^{-1}\mat{M\bf{e}_{r+1} & \cdots & M\bf{e}_n}\bf{x}&=\mat{M^{-1}M\bf{e}_{r+1} & \cdots & M^{-1}M\bf{e}_n}\bf{x} \\
&=\mat{\bf{e}_{r+1} & \cdots & \bf{e}_n}\bf{x} \\
&=M^{-1}\bf{0}=\bf{0}
\end{align}
Since $\bf{x}\neq\bf{0}$, this leads to that 
$\{\bf{e}_{r+1},\dots,\bf{e}_n\}$, a basis for $\opn{Nul}(AM)$, is linear dependent.

Therefore, $\{M\bf{e}_{r+1},\dots,M\bf{e}_n\}$ forms a basis for $\opn{Nul}A$. Specifically, these vectors are the $r+1$-th column to the $n$-th column of matrix $M$.

One problem remains: how to obtain $M$? Using the same logic of finding the inverse of a matrix, we augment the original matrix with an identity matrix below $A$.
$$\mat{A \\ I}\sim\mat{B \\ M}$$
As $A$ is reduced to the reduced column echelon form, the identity matrix is transformed to $M$.

### An Example

$$\mat{1 & 2 & 3 \\ 2 & 4 & 6 \\ 1 & 1 & 1}$$

::: {.solution}

## Row Operations

$$\mat{1 & 2 & 3 \\ 2 & 4 & 6 \\ 1 & 1 & 1}\sim \mat{1 & 0 & -1 \\ 0 & 1 & 2 \\ 0 & 0 & 0}$$

Therefore, $\left\{\mat{1 \\ -2 \\ 1}\right\}$ is a basis for $\opn{Nul}A$.
:::

::: {.solution}

## Column Operations

$$\mat{1 & 2 & 3 \\ 2 & 4 & 6 \\ 1 & 1 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1}\sim\mat{1 & 0 & 0 \\ 2 & 0 & 0 \\ 1 & -1 & -2 \\ 1 & -2 & -3 \\ 0 & 1 & 0 \\ 0 & 0 & 1}\sim\mat{1 & 0 & 0 \\ 2 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 2 & 1 \\ 0 & -1 & -2 \\ 0 & 0 & 1}$$

The third column $\mat{1 \\ -2 \\ 1}$ gives the exactly same result as what we obtain using row operations.
:::

::: {.remark}

In fact, we may only reduce $A$ to the column echelon form since all operations required to transform a matrix from its column echelon form to its reduced column echelon form do not change the $r+1$-th to the $n$-th columns of $M$.
:::